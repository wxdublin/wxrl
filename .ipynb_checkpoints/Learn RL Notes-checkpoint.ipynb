{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes\n",
    "\n",
    "# Books\n",
    "\n",
    "# 1 Intro to RL\n",
    "## 1.1 Many faces of RL\n",
    "<img src=\"images/rl-faces.png\" width=600 />\n",
    "## 1.2 Terminology \n",
    "### Reward, Reward hypothesis\n",
    "<img src=\"images/rl-reward.png\" width=600 />\n",
    "### Total reward, sequential decision making\n",
    "<img src=\"images/rl-sequence.png\" width=600 />\n",
    "### Agent, Environment, History, State\n",
    "<img src=\"images/rl-agent-environment.png\" width=600 />\n",
    "<img src=\"images/rl-history-state.png\" width=600 />\n",
    "### Environment state, Agent state, Information state\n",
    "<img src=\"images/rl-environment-state.png\" width=600 />\n",
    "<img src=\"images/rl-agent-state.png\" width=600 />\n",
    "<img src=\"images/rl-information-state.png\" width=600 />\n",
    "\n",
    "## 1.3 MDP, FOMDP\n",
    "<img src=\"images/rl-mdp.png\" width=600 />\n",
    "<img src=\"images/rl-fomdp.png\" width=600 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Major components of RL agent\n",
    "### 1.4.1 Policy: agent's behavior function\n",
    "> A policy is the agent's behavior\n",
    "\n",
    "> it's a map from state to action\n",
    "\n",
    "#### Deterministric Policy\n",
    "$$ a = \\pi(s) $$\n",
    "#### Stochastic Policy\n",
    "$$ \\pi(a|s) = P[A_{t} = a | S_{t} = s] $$\n",
    "<img src=\"images/maze.png\" width=600 />\n",
    "<img src=\"images/maze-policy.png\" width=600 />\n",
    "\n",
    "### 1.4.2 Value function: how good is each state and/or action\n",
    "> Value function is a prediction of the future reward\n",
    "\n",
    "> Used to evaluate the goodness/badness of states\n",
    "<img src=\"images/maze-value-function.png\" width=600 />\n",
    "\n",
    "Therefore need to select between actions, e.g.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\lambda R_{t+2}+\\lambda^{2}_{t+3}+...| S_{t} = s] $$\n",
    "\n",
    "### 1.4.3 Model: agent's representation of environment\n",
    "> A Model predicts what the environment will do next\n",
    "<img src=\"images/maze-model.png\" width=600 />\n",
    "\n",
    "#### $\\mathcal{P}$ predicts next state\n",
    "$$ \\mathcal{P}^{a}_{ss'}=P[S_{t+1}=s'|S_{t}=s, A_{t}=a]$$\n",
    "#### $\\mathcal{R}$ predicts next (intermediate) reward\n",
    "$$ \\mathcal{R}^{a}_{s} = E[R_{t+1}|S_{t}=s, A_{t}=a] $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 RL Agent Taxonomy\n",
    "<img src=\"images/rl-taxonomy.png\" width=600 />\n",
    "\n",
    "## 1.6 Learning and Planning\n",
    "Two fundamental problems in sequential decision making\n",
    "### Reinforcement Learning:\n",
    "* The environment is initially unknown\n",
    "* The agent interacts with the environment\n",
    "* The agent improves its policy\n",
    "<img src=\"images/rl-atari-learning.png\" width=600 />\n",
    "### Planning:\n",
    "* A model of the environment is known\n",
    "* The agent performs computations with its model (without any external interaction)\n",
    "* The agent improves its policy\n",
    "* a.k.a. deliberation, reasoning, introspection, pondering, thought, search\n",
    "<img src=\"images/rl-atari-planing.png\" width=600 />\n",
    "\n",
    "## 1.7 Exploration and Exploitation\n",
    "* Reinforcement learning is like trial-and-error learning\n",
    "* The agent should discover a good policy\n",
    "* From its experiences of the environment\n",
    "* Without losing too much reward along the way\n",
    "\n",
    "* Exploration finds more information about the environment\n",
    "* Exploitation exploits known information to maximise reward\n",
    "* It is usually important to explore as well as exploit\n",
    "## Examples\n",
    "* Restaurant Selection\n",
    "    * Exploitation Go to your favourite restaurant\n",
    "    * Exploration Try a new restaurant\n",
    "* Online Banner Advertisements\n",
    "    * Exploitation Show the most successful advert\n",
    "    * Exploration Show a different advert\n",
    "* Oil Drilling\n",
    "    * Exploitation Drill at the best known location\n",
    "    * Exploration Drill at a new location\n",
    "* Game Playing\n",
    "    * Exploitation Play the move you believe is best\n",
    "    * Exploration Play an experimental move\n",
    "\n",
    "## 1.8 Prediction and Control\n",
    "* Prediction: evaluate the future\n",
    "    * Given a policy\n",
    "* Control: optimise the future\n",
    "    * Find the best policy\n",
    "<img src=\"images/rl-gridworld-prediction.png\" width=600 />\n",
    "<img src=\"images/rl-gridworld-control.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Markov Decision Processes\n",
    "\n",
    "## 2.1 Markov Processes\n",
    "* Markov decision processes formally describe an environment for reinforcement learning\n",
    "* Where the environment is **fully observable**\n",
    "* i.e. The current state completely characterises the process\n",
    "* Almost all RL problems can be formalised as MDPs, e.g. \n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * **Partially observable** problems can be converted into MDPs\n",
    "    * **Bandits** are MDPs with one state\n",
    "    \n",
    "### 2.1.2 Markov Property\n",
    "> “The future is independent of the past given the present”\n",
    "\n",
    "A state $S_{t}$ is Markov if and only if $$P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1}, S_{2}. ..., S_{t}]$$ \n",
    "* The **state** captures all relevant information from the **history**\n",
    "* Once the state is known, the history may be thrown away\n",
    "* i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "### 2.1.3 State Transition Matrix\n",
    "For a Markov state s and successor state s , the **state transition probability** is defined by\n",
    "$$ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$$\n",
    "**State transition matrix** $\\mathcal{P}$ defines transition probabilities from all states s to all successor states s',\n",
    "$$ \\mathcal{P} = \\begin{bmatrix} P_{11} & \\dots & P_{1n} \\\\\n",
    "\\dots \\\\\n",
    "P_{n1} & \\dots & P_{nn}\\end{bmatrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "### 2.1.4 Markov Process\n",
    "> A Markov process is a memoryless random process, i.e. a sequence of random states $S_{1} , S_{2}$ , ... with the Markov property.\n",
    "\n",
    "A Markov Process (or Markov Chain) is a tuple (S, P)\n",
    "* S is a (finite) set of states\n",
    "* P is a state transition probability matrix, $ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "<img src=\"images/rl-markov.png\" width=600 />\n",
    "<img src=\"images/rl-markov-episodes.png\" width=600 />\n",
    "<img src=\"images/rl-markov-state-transition-matrix.png\" width=600 />\n",
    "\n",
    "## 2.2 Markov Reward Processes\n",
    "> A Markov reward process is a Markov chain with values.\n",
    "\n",
    "A Markov Reward Process is a tuple $(S, P, R, \\gamma)$\n",
    "* S is a finite set of states\n",
    "* P is a state transition probability matrix, $P_{ss'} = P [S_{t+1} = s' | S_{t} = s]$\n",
    "* R is a reward function, $R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma$ ∈ [0, 1]\n",
    "\n",
    "### Return\n",
    "The return G t is the total discounted reward from time-step t.\n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1}$$\n",
    "## 2.3 Markov Decision Processes\n",
    "## 2.4 Extensions to MDPs\n",
    "\n",
    "# 3 Planning by Dynamic Programming\n",
    "# 4 Model-Free Prediction\n",
    "# 5 Model-Free Control\n",
    "# 6 Value Function Approximation\n",
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
