{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes\n",
    "\n",
    "# Books\n",
    "\n",
    "# 1 Intro to RL\n",
    "## 1.1 Many faces of RL\n",
    "<img src=\"images/rl-faces.png\" width=600 />\n",
    "## 1.2 Terminology \n",
    "### Reward, Reward hypothesis\n",
    "<img src=\"images/rl-reward.png\" width=600 />\n",
    "### Total reward, sequential decision making\n",
    "<img src=\"images/rl-sequence.png\" width=600 />\n",
    "### Agent, Environment, History, State\n",
    "<img src=\"images/rl-agent-environment.png\" width=600 />\n",
    "<img src=\"images/rl-history-state.png\" width=600 />\n",
    "### Environment state, Agent state, Information state\n",
    "<img src=\"images/rl-environment-state.png\" width=600 />\n",
    "<img src=\"images/rl-agent-state.png\" width=600 />\n",
    "<img src=\"images/rl-information-state.png\" width=600 />\n",
    "\n",
    "## 1.3 MDP, FOMDP\n",
    "<img src=\"images/rl-mdp.png\" width=600 />\n",
    "<img src=\"images/rl-fomdp.png\" width=600 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Major components of RL agent\n",
    "### 1.4.1 Policy: agent's behavior function\n",
    "> A policy is the agent's behavior\n",
    "\n",
    "> it's a map from state to action\n",
    "\n",
    "#### Deterministric Policy\n",
    "$$ a = \\pi(s) $$\n",
    "#### Stochastic Policy\n",
    "$$ \\pi(a|s) = P[A_{t} = a | S_{t} = s] $$\n",
    "<img src=\"images/maze.png\" width=600 />\n",
    "<img src=\"images/maze-policy.png\" width=600 />\n",
    "\n",
    "### 1.4.2 Value function: how good is each state and/or action\n",
    "> Value function is a prediction of the future reward\n",
    "\n",
    "> Used to evaluate the goodness/badness of states\n",
    "<img src=\"images/maze-value-function.png\" width=600 />\n",
    "\n",
    "Therefore need to select between actions, e.g.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\lambda R_{t+2}+\\lambda^{2}_{t+3}+...| S_{t} = s] $$\n",
    "\n",
    "### 1.4.3 Model: agent's representation of environment\n",
    "> A Model predicts what the environment will do next\n",
    "<img src=\"images/maze-model.png\" width=600 />\n",
    "\n",
    "#### $\\mathcal{P}$ predicts next state\n",
    "$$ \\mathcal{P}^{a}_{ss'}=P[S_{t+1}=s'|S_{t}=s, A_{t}=a]$$\n",
    "#### $\\mathcal{R}$ predicts next (intermediate) reward\n",
    "$$ \\mathcal{R}^{a}_{s} = E[R_{t+1}|S_{t}=s, A_{t}=a] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 RL Agent Taxonomy\n",
    "<img src=\"images/rl-taxonomy.png\" width=600 />\n",
    "\n",
    "## 1.6 Learning and Planning\n",
    "Two fundamental problems in sequential decision making\n",
    "### Reinforcement Learning:\n",
    "* The environment is initially unknown\n",
    "* The agent interacts with the environment\n",
    "* The agent improves its policy\n",
    "<img src=\"images/rl-atari-learning.png\" width=600 />\n",
    "### Planning:\n",
    "* A model of the environment is known\n",
    "* The agent performs computations with its model (without any external interaction)\n",
    "* The agent improves its policy\n",
    "* a.k.a. deliberation, reasoning, introspection, pondering, thought, search\n",
    "<img src=\"images/rl-atari-planing.png\" width=600 />\n",
    "\n",
    "## 1.7 Exploration and Exploitation\n",
    "* Reinforcement learning is like trial-and-error learning\n",
    "* The agent should discover a good policy\n",
    "* From its experiences of the environment\n",
    "* Without losing too much reward along the way\n",
    "\n",
    "* Exploration finds more information about the environment\n",
    "* Exploitation exploits known information to maximise reward\n",
    "* It is usually important to explore as well as exploit\n",
    "## Examples\n",
    "* Restaurant Selection\n",
    "    * Exploitation Go to your favourite restaurant\n",
    "    * Exploration Try a new restaurant\n",
    "* Online Banner Advertisements\n",
    "    * Exploitation Show the most successful advert\n",
    "    * Exploration Show a different advert\n",
    "* Oil Drilling\n",
    "    * Exploitation Drill at the best known location\n",
    "    * Exploration Drill at a new location\n",
    "* Game Playing\n",
    "    * Exploitation Play the move you believe is best\n",
    "    * Exploration Play an experimental move\n",
    "\n",
    "## 1.8 Prediction and Control\n",
    "* Prediction: evaluate the future\n",
    "    * Given a policy\n",
    "* Control: optimise the future\n",
    "    * Find the best policy\n",
    "<img src=\"images/rl-gridworld-prediction.png\" width=600 />\n",
    "<img src=\"images/rl-gridworld-control.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Markov Decision Processes\n",
    "\n",
    "## 2.1 Markov Processes\n",
    "* Markov decision processes formally describe an environment for reinforcement learning\n",
    "* Where the environment is **fully observable**\n",
    "* i.e. The current state completely characterises the process\n",
    "* Almost all RL problems can be formalised as MDPs, e.g. \n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * **Partially observable** problems can be converted into MDPs\n",
    "    * **Bandits** are MDPs with one state\n",
    "    \n",
    "### 2.1.2 Markov Property\n",
    "> “The future is independent of the past given the present”\n",
    "\n",
    "A state $S_{t}$ is Markov if and only if $$P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1}, S_{2}. ..., S_{t}]$$ \n",
    "* The **state** captures all relevant information from the **history**\n",
    "* Once the state is known, the history may be thrown away\n",
    "* i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "### 2.1.3 State Transition Matrix\n",
    "For a Markov state s and successor state s , the **state transition probability** is defined by\n",
    "$$ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$$\n",
    "**State transition matrix** $\\mathcal{P}$ defines transition probabilities from all states s to all successor states s',\n",
    "$$ \\mathcal{P} = \\begin{bmatrix} P_{11} & \\dots & P_{1n} \\\\\n",
    "\\dots \\\\\n",
    "P_{n1} & \\dots & P_{nn}\\end{bmatrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "### 2.1.4 Markov Process\n",
    "> A Markov process is a memoryless random process, i.e. a sequence of random states $S_{1} , S_{2}$ , ... with the Markov property.\n",
    "\n",
    "A Markov Process (or Markov Chain) is a tuple (S, P)\n",
    "* S is a (finite) set of states\n",
    "* P is a state transition probability matrix, $ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "<img src=\"images/rl-markov.png\" width=600 />\n",
    "<img src=\"images/rl-markov-episodes.png\" width=600 />\n",
    "<img src=\"images/rl-markov-state-transition-matrix.png\" width=600 />\n",
    "\n",
    "## 2.2 Markov Reward Processes\n",
    "> A Markov reward process is a Markov chain with values.\n",
    "\n",
    "A Markov Reward Process is a tuple $(S, P, R, \\gamma)$\n",
    "* S is a finite set of states\n",
    "* P is a state transition probability matrix, $P_{ss'} = P [S_{t+1} = s' | S_{t} = s]$\n",
    "* R is a reward function, $R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma$ ∈ [0, 1]\n",
    "\n",
    "### 2.2.1 Return\n",
    "The return $G_{t}$ is the total discounted reward from time-step t.\n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1}$$\n",
    "* The discount γ ∈ [0, 1] is the present value of future rewards \n",
    "* The value of receiving reward R after k + 1 time-steps is $γ^{k} R$ . \n",
    "* This values immediate reward above delayed reward.\n",
    "    * γ close to 0 leads to ”myopic” evaluation\n",
    "    * γ close to 1 leads to ”far-sighted” evaluation\n",
    "\n",
    "### 2.2.2 Value Function\n",
    "> The value function v(s) gives the long-term value of state s\n",
    "\n",
    "The state value function v(s) of an MRP is the expected return starting from state s\n",
    "$$ v(s) = E[G_{t}|S_{t} = s]$$\n",
    "\n",
    "### 2.2.3 Example MRP\n",
    "<img src=\"images/rl-mrp-returns.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma0.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma1.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma2.png\" width=600 />\n",
    "\n",
    "### 2.2.4 Bellman Equation for MRPs\n",
    "The value function can be decomposed into two parts: \n",
    "* immediate reward $R_{t+1}$\n",
    "* discounted value of successor state $γv(S_{t+1})$\n",
    "<img src=\"images/rl-bellman.png\" width=600 />\n",
    "#### 2.2.4.1 Example MRP Bellman\n",
    "<img src=\"images/rl-bellman-example.png\" width=600 />\n",
    "#### 2.2.4.2 Bellman in matrix form\n",
    "<img src=\"images/rl-bellman-matrix.png\" width=600 />\n",
    "#### 2.2.4.3 Solving Bellman Equation\n",
    "<img src=\"images/rl-bellman-solving.png\" width=600 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Markov Decision Processes\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "A Markov Decision Process is a tuple ⟨S, A, P, R, γ⟩ \n",
    "* S is a finite set of states\n",
    "* A is a finite set of actions\n",
    "* P is a state transition probability matrix,\n",
    "* $P^{a}_{ss′}\\ =\\ P[S_{t+1}=s′|S_{t}=s,\\ A_{t}=a] $\n",
    "* R is a reward function, $R^{a}_{s} = E[R_{t+1}\\ | S_{t} = s,\\ A_{t} = a] $\n",
    "* γ is a discount factor γ ∈ [0, 1].\n",
    "\n",
    "<img src=\"images/rl-mdp-example.png\" width=600 />\n",
    "\n",
    "\n",
    "### 2.3.1 Policies \n",
    "A policy π is a distribution over actions given states, $$π(a|s)=P[A_{t} =a|S_{t} =s]$$\n",
    "* A policy fully defines the behaviour of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are stationary (time-independent), $$A_{t}\\ ∼\\ π(·|S_{t}),∀t>0$$\n",
    "\n",
    "* Given an MDP M = ⟨S,A,P,R,γ⟩ and a policy π\n",
    "* The state sequence S1, S2, ... is a Markov process $⟨S, P^{π}⟩$\n",
    "* The state and reward sequence S1, R2, S2, ... is a Markov reward process $⟨S, P^{π}, R^{π}, γ⟩$\n",
    "* where \n",
    "\n",
    "$$ P^{\\pi}_{s,s'} = \\sum_{a \\in A}\\pi(a|s)P^a_{ss'}$$\n",
    "$$ R^{\\pi}_{s} = \\sum_{a \\in A}\\pi(a|s)R^{a}_{s} $$\n",
    "\n",
    "### 2.3.2 Value function\n",
    "\n",
    "The **state-value function $v_{π}(s)$** of an MDP is the expected return starting from state s, and then following policy π\n",
    "$$v_{π}(s)=E_{π}[G_{t} |S_{t} =s]$$\n",
    "\n",
    "The **action-value function $q_{π}(s,a)$** is the expected return starting from state s, taking action a, and then following policy π\n",
    "$$q_{π}(s,a)=E_{π}[G_{t} |S_{t} =s,A_{t} =a]$$\n",
    "\n",
    "<img src=\"images/rl-mdp-state-value.png\" width=600 />\n",
    "\n",
    "### 2.3.3 Bellman Expectation Equation\n",
    "\n",
    "\n",
    "## 2.4 Extensions to MDPs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3 Planning by Dynamic Programming\n",
    "# 4 Model-Free Prediction\n",
    "# 5 Model-Free Control\n",
    "# 6 Value Function Approximation\n",
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
