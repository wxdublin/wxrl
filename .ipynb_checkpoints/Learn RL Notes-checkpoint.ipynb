{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes\n",
    "\n",
    "# Books\n",
    "\n",
    "# 1 Intro to RL\n",
    "## 1.1 Many faces of RL\n",
    "<img src=\"images/rl-faces.png\" width=600 />\n",
    "## 1.2 Terminology \n",
    "### Reward, Reward hypothesis\n",
    "<img src=\"images/rl-reward.png\" width=600 />\n",
    "### Total reward, sequential decision making\n",
    "<img src=\"images/rl-sequence.png\" width=600 />\n",
    "### Agent, Environment, History, State\n",
    "<img src=\"images/rl-agent-environment.png\" width=600 />\n",
    "<img src=\"images/rl-history-state.png\" width=600 />\n",
    "### Environment state, Agent state, Information state\n",
    "<img src=\"images/rl-environment-state.png\" width=600 />\n",
    "<img src=\"images/rl-agent-state.png\" width=600 />\n",
    "<img src=\"images/rl-information-state.png\" width=600 />\n",
    "\n",
    "## 1.3 MDP, FOMDP\n",
    "<img src=\"images/rl-mdp.png\" width=600 />\n",
    "<img src=\"images/rl-fomdp.png\" width=600 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Major components of RL agent\n",
    "### 1.4.1 Policy: agent's behavior function\n",
    "> A policy is the agent's behavior\n",
    "\n",
    "> it's a map from state to action\n",
    "\n",
    "#### Deterministric Policy\n",
    "$$ a = \\pi(s) $$\n",
    "#### Stochastic Policy\n",
    "$$ \\pi(a|s) = P[A_{t} = a | S_{t} = s] $$\n",
    "<img src=\"images/maze.png\" width=600 />\n",
    "<img src=\"images/maze-policy.png\" width=600 />\n",
    "\n",
    "### 1.4.2 Value function: how good is each state and/or action\n",
    "> Value function is a prediction of the future reward\n",
    "\n",
    "> Used to evaluate the goodness/badness of states\n",
    "<img src=\"images/maze-value-function.png\" width=600 />\n",
    "\n",
    "Therefore need to select between actions, e.g.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\lambda R_{t+2}+\\lambda^{2}_{t+3}+...| S_{t} = s] $$\n",
    "\n",
    "### 1.4.3 Model: agent's representation of environment\n",
    "> A Model predicts what the environment will do next\n",
    "<img src=\"images/maze-model.png\" width=600 />\n",
    "\n",
    "#### $\\mathcal{P}$ predicts next state\n",
    "$$ \\mathcal{P}^{a}_{ss'}=P[S_{t+1}=s'|S_{t}=s, A_{t}=a]$$\n",
    "#### $\\mathcal{R}$ predicts next (intermediate) reward\n",
    "$$ \\mathcal{R}^{a}_{s} = E[R_{t+1}|S_{t}=s, A_{t}=a] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 RL Agent Taxonomy\n",
    "<img src=\"images/rl-taxonomy.png\" width=600 />\n",
    "\n",
    "## 1.6 Learning and Planning\n",
    "Two fundamental problems in sequential decision making\n",
    "### Reinforcement Learning:\n",
    "* The environment is initially unknown\n",
    "* The agent interacts with the environment\n",
    "* The agent improves its policy\n",
    "<img src=\"images/rl-atari-learning.png\" width=600 />\n",
    "### Planning:\n",
    "* A model of the environment is known\n",
    "* The agent performs computations with its model (without any external interaction)\n",
    "* The agent improves its policy\n",
    "* a.k.a. deliberation, reasoning, introspection, pondering, thought, search\n",
    "<img src=\"images/rl-atari-planing.png\" width=600 />\n",
    "\n",
    "## 1.7 Exploration and Exploitation\n",
    "* Reinforcement learning is like trial-and-error learning\n",
    "* The agent should discover a good policy\n",
    "* From its experiences of the environment\n",
    "* Without losing too much reward along the way\n",
    "\n",
    "* Exploration finds more information about the environment\n",
    "* Exploitation exploits known information to maximise reward\n",
    "* It is usually important to explore as well as exploit\n",
    "## Examples\n",
    "* Restaurant Selection\n",
    "    * Exploitation Go to your favourite restaurant\n",
    "    * Exploration Try a new restaurant\n",
    "* Online Banner Advertisements\n",
    "    * Exploitation Show the most successful advert\n",
    "    * Exploration Show a different advert\n",
    "* Oil Drilling\n",
    "    * Exploitation Drill at the best known location\n",
    "    * Exploration Drill at a new location\n",
    "* Game Playing\n",
    "    * Exploitation Play the move you believe is best\n",
    "    * Exploration Play an experimental move\n",
    "\n",
    "## 1.8 Prediction and Control\n",
    "* Prediction: evaluate the future\n",
    "    * Given a policy\n",
    "* Control: optimise the future\n",
    "    * Find the best policy\n",
    "<img src=\"images/rl-gridworld-prediction.png\" width=600 />\n",
    "<img src=\"images/rl-gridworld-control.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Markov Decision Processes\n",
    "\n",
    "## 2.1 Markov Processes\n",
    "* Markov decision processes formally describe an environment for reinforcement learning\n",
    "* Where the environment is **fully observable**\n",
    "* i.e. The current state completely characterises the process\n",
    "* Almost all RL problems can be formalised as MDPs, e.g. \n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * **Partially observable** problems can be converted into MDPs\n",
    "    * **Bandits** are MDPs with one state\n",
    "    \n",
    "### 2.1.2 Markov Property\n",
    "> “The future is independent of the past given the present”\n",
    "\n",
    "A state $S_{t}$ is Markov if and only if $$P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1}, S_{2}. ..., S_{t}]$$ \n",
    "* The **state** captures all relevant information from the **history**\n",
    "* Once the state is known, the history may be thrown away\n",
    "* i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "### 2.1.3 State Transition Matrix\n",
    "For a Markov state s and successor state s , the **state transition probability** is defined by\n",
    "$$ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$$\n",
    "**State transition matrix** $\\mathcal{P}$ defines transition probabilities from all states s to all successor states s',\n",
    "$$ \\mathcal{P} = \\begin{bmatrix} P_{11} & \\dots & P_{1n} \\\\\n",
    "\\dots \\\\\n",
    "P_{n1} & \\dots & P_{nn}\\end{bmatrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "### 2.1.4 Markov Process\n",
    "> A Markov process is a memoryless random process, i.e. a sequence of random states $S_{1} , S_{2}$ , ... with the Markov property.\n",
    "\n",
    "A Markov Process (or Markov Chain) is a tuple (S, P)\n",
    "* S is a (finite) set of states\n",
    "* P is a state transition probability matrix, $ \\mathcal{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "<img src=\"images/rl-markov.png\" width=600 />\n",
    "<img src=\"images/rl-markov-episodes.png\" width=600 />\n",
    "<img src=\"images/rl-markov-state-transition-matrix.png\" width=600 />\n",
    "\n",
    "## 2.2 Markov Reward Processes\n",
    "> A Markov reward process is a Markov chain with values.\n",
    "\n",
    "A Markov Reward Process is a tuple $(S, P, R, \\gamma)$\n",
    "* S is a finite set of states\n",
    "* P is a state transition probability matrix, $P_{ss'} = P [S_{t+1} = s' | S_{t} = s]$\n",
    "* R is a reward function, $R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma$ ∈ [0, 1]\n",
    "\n",
    "### 2.2.1 Return\n",
    "The return $G_{t}$ is the total discounted reward from time-step t.\n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1}$$\n",
    "* The discount γ ∈ [0, 1] is the present value of future rewards \n",
    "* The value of receiving reward R after k + 1 time-steps is $γ^{k} R$ . \n",
    "* This values immediate reward above delayed reward.\n",
    "    * γ close to 0 leads to ”myopic” evaluation\n",
    "    * γ close to 1 leads to ”far-sighted” evaluation\n",
    "\n",
    "### 2.2.2 Value Function\n",
    "> The value function v(s) gives the long-term value of state s\n",
    "\n",
    "The state value function v(s) of an MRP is the expected return starting from state s\n",
    "$$ v(s) = E[G_{t}|S_{t} = s]$$\n",
    "\n",
    "### 2.2.3 Example MRP\n",
    "<img src=\"images/rl-mrp-returns.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma0.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma1.png\" width=600 />\n",
    "<img src=\"images/rl-mrp-gamma2.png\" width=600 />\n",
    "\n",
    "### 2.2.4 Bellman Equation for MRPs\n",
    "The value function can be decomposed into two parts: \n",
    "* immediate reward $R_{t+1}$\n",
    "* discounted value of successor state $γv(S_{t+1})$\n",
    "<img src=\"images/rl-bellman.png\" width=600 />\n",
    "#### 2.2.4.1 Example MRP Bellman\n",
    "<img src=\"images/rl-bellman-example.png\" width=600 />\n",
    "#### 2.2.4.2 Bellman in matrix form\n",
    "<img src=\"images/rl-bellman-matrix.png\" width=600 />\n",
    "#### 2.2.4.3 Solving Bellman Equation\n",
    "<img src=\"images/rl-bellman-solving.png\" width=600 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Markov Decision Processes\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "A Markov Decision Process is a tuple ⟨S, A, P, R, γ⟩ \n",
    "* S is a finite set of states\n",
    "* A is a finite set of actions\n",
    "* P is a state transition probability matrix,\n",
    "* $P^{a}_{ss′}\\ =\\ P[S_{t+1}=s′|S_{t}=s,\\ A_{t}=a] $\n",
    "* R is a reward function, $R^{a}_{s} = E[R_{t+1}\\ | S_{t} = s,\\ A_{t} = a] $\n",
    "* γ is a discount factor γ ∈ [0, 1].\n",
    "\n",
    "<img src=\"images/rl-mdp-example.png\" width=600 />\n",
    "\n",
    "\n",
    "### 2.3.1 Policies \n",
    "A policy π is a distribution over actions given states, $$π(a|s)=P[A_{t} =a|S_{t} =s]$$\n",
    "* A policy fully defines the behaviour of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are stationary (time-independent), $$A_{t}\\ ∼\\ π(·|S_{t}),∀t>0$$\n",
    "\n",
    "* Given an MDP M = ⟨S,A,P,R,γ⟩ and a policy π\n",
    "* The state sequence S1, S2, ... is a Markov process $⟨S, P^{π}⟩$\n",
    "* The state and reward sequence S1, R2, S2, ... is a Markov reward process $⟨S, P^{π}, R^{π}, γ⟩$\n",
    "* where \n",
    "\n",
    "$$ P^{\\pi}_{s,s'} = \\sum_{a \\in A}\\pi(a|s)P^a_{ss'}$$\n",
    "$$ R^{\\pi}_{s} = \\sum_{a \\in A}\\pi(a|s)R^{a}_{s} $$\n",
    "\n",
    "### 2.3.2 Value function\n",
    "\n",
    "The **state-value function $v_{π}(s)$** of an MDP is the expected return starting from state s, and then following policy π\n",
    "$$v_{π}(s)=E_{π}[G_{t} |S_{t} =s]$$\n",
    "\n",
    "The **action-value function $q_{π}(s,a)$** is the expected return starting from state s, taking action a, and then following policy π\n",
    "$$q_{π}(s,a)=E_{π}[G_{t} |S_{t} =s,A_{t} =a]$$\n",
    "\n",
    "<img src=\"images/rl-mdp-state-value.png\" width=600 />\n",
    "\n",
    "### 2.3.3 Bellman Expectation Equation\n",
    "The state-value function can again be decomposed into immediate reward plus discounted value of successor state,\n",
    "$$ v_{\\pi}(s) = E_{\\pi} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\ |\\ S_{t} = s]$$\n",
    "\n",
    "The action-value function can similarly be decomposed,\n",
    "$$ q_{\\pi}(s, a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})\\ |\\ S_{t} = s, A_{t} = a]$$\n",
    "\n",
    "**Bellman Expectation Equation for $V_{π}$ and $Q_{π}$**\n",
    "> state value\n",
    "<img src=\"images/rl-mdp-vpi.png\" width=600/> \n",
    "> action value\n",
    "<img src=\"images/rl-mdp-qpi.png\" width=600/>\n",
    "> state value2\n",
    "<img src=\"images/rl-mdp-vpi2.png\" width=600/>\n",
    "> action value2\n",
    "<img src=\"images/rl-mdp-qpi2.png\" width=600/>\n",
    "\n",
    "** Example**\n",
    "<img src=\"images/rl-mdp-bellman-example.png\" width=600/>\n",
    "\n",
    "** Bellman Expectation Equation (Matrix Form) **\n",
    "<img src=\"images/rl-mdp-bellman-matrix-form.png\" width=600/>\n",
    "\n",
    "### 2.3.4 Optimal Value Function\n",
    "The **optimal state-value function** $v_{∗}(s)$ is the maximum value function over all policies\n",
    "$$ v_{∗}(s) = \\underset{\\pi}{max}\\ v_{\\pi} (s)$$\n",
    "\n",
    "The **optimal action-value function** $q_{∗} (s, a)$ is the maximum action-value function over all policies\n",
    "$$ q_{∗} (s, a) = \\underset{\\pi}{max}\\ q_{\\pi} (s, a)$$\n",
    "\n",
    "\n",
    "* The optimal value function specifies the best possible performance in the MDP.\n",
    "* An MDP is “solved” when we know the optimal value fn.\n",
    "\n",
    "<img src=\"images/rl-mdp-optimal-state.png\" width=600/>\n",
    "<img src=\"images/rl-mdp-optimal-action.png\" width=600/>\n",
    "\n",
    "### 2.3.5 Optimal policy\n",
    "\n",
    "Define a **partial ordering over policies**\n",
    "$$π ≥ π' if v_{π} (s') ≥ v_{π} (s), ∀s$$\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "For any Markov Decision Process\n",
    "* There exists an optimal policy $π_{∗}$ that is better than or equal to all other policies, $π_{∗} ≥ π, ∀π$\n",
    "* All optimal policies achieve the optimal value function, $v_{π_{∗}} (s) = v_{∗} (s)$\n",
    "* All optimal policies achieve the optimal action-value function, $q_{π_{∗}} (s, a) = q_{∗} (s, a)$\n",
    "\n",
    "<img src=\"images/rl-mdp-optimal-policy.png\" width=600/>\n",
    "\n",
    "### 2.3.6 Finding an Optimal Policy\n",
    "An optimal policy can be found by maximising over $q_{∗} (s, a)$,\n",
    "$$ π_{∗} (a|s) = \\ \n",
    "\\begin{cases}\n",
    "1\\ if a = \\underset{a \\in A}{argmax}\\ q_{∗} (s, a) \\\\\n",
    "0\\ otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* There is always a deterministic optimal policy for any MDP \n",
    "* If we know $q_{∗} (s, a)$, we immediately have the optimal policy\n",
    "\n",
    "### 2.3.7 Bellman Optimality Equation\n",
    "** Bellman Optimality Equation for $v_{∗}$ and $Q_{∗}$ **\n",
    "> state value\n",
    "<img src=\"images/rl-mdp-optimality-state.png\" width=600 />\n",
    "> action value\n",
    "<img src=\"images/rl-mdp-optimality-action.png\" width=600/>\n",
    "> state value2\n",
    "<img src=\"images/rl-mdp-optimality-state2.png\" width=600/>\n",
    "> action value 2\n",
    "<img src=\"images/rl-mdp-optimality-action2.png\" width=600/>\n",
    "** Example**\n",
    "<img src=\"images/rl-mdp-optimality-example.png\" width=700/>\n",
    "\n",
    "### 2.3.8 Solving the Bellman Optimality Equation\n",
    "* Bellman Optimality Equation is **non-linear**\n",
    "* **No closed form solution (in general)**\n",
    "* Many iterative solution methods\n",
    "    * **Value Iteration**\n",
    "    * **Policy Iteration**\n",
    "    * **Q-learning**\n",
    "    * **Sarsa**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions to MDPs\n",
    "\n",
    "### 2.4.1 Infinite and continuous MDPs\n",
    "The following extensions are all possible:\n",
    "* Countably infinite state and/or action spaces\n",
    "    * Straightforward\n",
    "* Continuous state and/or action spaces\n",
    "    * Closed form for linear quadratic model (LQR)\n",
    "* Continuous time\n",
    "    * Requires partial differential equations\n",
    "    * Hamilton-Jacobi-Bellman (HJB) equation\n",
    "    * Limiting case of Bellman equation as time-step → 0\n",
    "    \n",
    "### 2.4.2 Partially observable MDPs (POMDP)\n",
    "A Partially Observable Markov Decision Process is an MDP with **hidden states**. It is a **hidden Markov model** with actions.\n",
    "\n",
    "Definition\n",
    "A POMDP is a tuple $(S, A, O, P, R, Z, \\gamma)$\n",
    "* S is a finite set of states\n",
    "* A is a finite set of actions\n",
    "* O is a finite set of observations\n",
    "* P is a state transition probability matrix, $P^{a}_{ss'}\\ =\\ P[S_{t+1} = s'| S_{t} = s, A_{t} = a]$\n",
    "* R is a reward function, $R^{a}_{s}\\ =\\ E [R_{t+1} | S_{t} = s, A_{t} = a]$\n",
    "* Z is an observation function,$Z^{a}_{s'o}\\ =\\ P [O_{t+1} = o | S_{t+1} = s' , A_{t} = a]$\n",
    "* $\\gamma$ is a discount factor $\\gamma \\in [0, 1]$.\n",
    "\n",
    "**Belief States**\n",
    "\n",
    "Definition\n",
    "A history $H_{t}$ is a sequence of actions, observations and rewards,\n",
    "$$H_{t} = A_{0} , O_{1} , R_{1} , ..., A_{t−1} , O_{t} , R_{t}$$\n",
    "\n",
    "Definition\n",
    "A belief state b(h) is a probability distribution over states, conditioned on the history h\n",
    "$$b(h) = (P[S_{t} = s^{1} | H_{t} = h] , ..., P [S_{t} = s^{n} | H_{t} = h])$$\n",
    "\n",
    "**Reductions of POMDPs**\n",
    "<img src=\"images/rl-pomdp-reduction.png\" width=600 />\n",
    "\n",
    "### 2.4.3 Undiscounted, average reward MDPs\n",
    "<img src=\"images/rl-ergodic-markov.png\" width=600 />\n",
    "<img src=\"images/rl-ergodic-markov2.png\" width=600 />\n",
    "<img src=\"images/rl-average-reward.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Planning by Dynamic Programming\n",
    "## 3.1 Introduction\n",
    "\n",
    "### What is Dynamic Programming?\n",
    "\n",
    "**Dynamic** sequential or temporal component to the problem\n",
    "\n",
    "**Programming** optimising a “program”, i.e. a policy\n",
    "    * c.f. linear programming\n",
    "\n",
    "* A method for solving complex problems\n",
    "* By breaking them down into **subproblems**\n",
    "* Solve the subproblems\n",
    "* Combine solutions to subproblems\n",
    "\n",
    "### Requirements for Dynamic Programming\n",
    "Dynamic Programming is a very general solution method for problems which have two properties:\n",
    "\n",
    "** Optimal substructure **\n",
    "* Principle of optimality applies\n",
    "* Optimal solution can be decomposed into subproblems\n",
    "\n",
    "** Overlapping subproblems **\n",
    "* Subproblems recur many times\n",
    "* Solutions can be cached and reused\n",
    "\n",
    "** Markov decision processes satisfy both properties **\n",
    "* Bellman equation gives recursive decomposition\n",
    "* Value function stores and reuses solutions\n",
    "\n",
    "### Planning by Dynamic Programming\n",
    "* Dynamic programming assumes **full knowledge** of the MDP\n",
    "* It is used for **planning** in an MDP\n",
    "* For **prediction**:\n",
    "    * Input: MDP (S, A, P, R, γ) and policy π\n",
    "    * or: MRP $(S, P^{π} , R^{π} , γ)$\n",
    "    * Output: value function $v_{π}$\n",
    "* Or for **control**:\n",
    "    * Input: MDP (S, A, P, R, γ)\n",
    "    * Output: optimal value function $v_{∗}$\n",
    "    *    and: optimal policy $π_{∗}$\n",
    "\n",
    "### Other Applications of Dynamic Programming\n",
    "Dynamic programming is used to solve many other problems, e.g.\n",
    "* Scheduling algorithms\n",
    "* String algorithms (e.g. sequence alignment)\n",
    "* Graph algorithms (e.g. shortest path algorithms)\n",
    "* Graphical models (e.g. Viterbi algorithm)\n",
    "* Bioinformatics (e.g. lattice models)\n",
    "\n",
    "## 3.2 Policy Evaluation\n",
    "### Iterative Policy Evaluation\n",
    "* Problem: evaluate a given policy π\n",
    "* Solution: iterative application of **Bellman expectation backup**\n",
    "* $v_{1} → v_{2} → ... → v_{π}$\n",
    "* Using **synchronous backups**,\n",
    "    * At each iteration k + 1\n",
    "    * For all states s ∈ S\n",
    "    * Update $v_{k+1}(s)$ from $v_{k}(s')$\n",
    "    * where s' is a successor state of s\n",
    "* We will discuss **asynchronous backups** later\n",
    "* Convergence to $v_{π}$ will be proven at the end of the lecture\n",
    "<img src=\"images/rl-dp-iterative-policy-evaluation.png\" width=600 />\n",
    "\n",
    "### Small Girdworld Example\n",
    "<img src=\"images/rl-dp-poleval-example1.png\" width=600 />\n",
    "<img src=\"images/rl-dp-poleval-example2.png\" width=600 />\n",
    "<img src=\"images/rl-dp-poleval-example3.png\" width=600 />\n",
    "\n",
    "## 3.3 Policy Iteration\n",
    "\n",
    "### How to Improve a Policy\n",
    "* Given a policy π\n",
    "    * Evaluate the policy π\n",
    "        $$ v_{π} (s) = E [R_{t+1} + γR_{t+2} + ...|S_{t} = s]$$\n",
    "    * Improve the policy by acting greedily with respect to $v_{π}$\n",
    "        $$ π' = greedy(v_{π} )$$\n",
    "* **In Small Gridworld improved policy was optimal, $π = π_{∗}$**\n",
    "* **In general, need more iterations of improvement / evaluation**\n",
    "* But this process of policy iteration always converges to π_{∗}\n",
    "\n",
    "### Greedy Policy Iteration\n",
    "<img src=\"images/rl-dp-policy-iteration.png\" width=600 />\n",
    "\n",
    "### Proof of Policy Improvement\n",
    "<img src=\"images/rl-dp-policy-improvement-proof.png\" width=600 />\n",
    "<img src=\"images/rl-dp-policy-improvement-proof2.png\" width=600 />\n",
    "\n",
    "### Modified Policy Iteration\n",
    "* Does policy evaluation need to converge to $v_{π}$ ?\n",
    "* Or should we introduce a stopping condition\n",
    "    * e.g. $\\epsilon -convergence$ of value function\n",
    "* Or simply stop after k iterations of iterative policy evaluation?\n",
    "    * For example, in the small gridworld k = 3 was sufficient to achieve optimal policy\n",
    "* Why not update policy every iteration? i.e. stop after k = 1\n",
    "    * This is equivalent to value iteration (next section)\n",
    "\n",
    "### Generalized Policy Iteration\n",
    "<img src=\"images/rl-dp-generalized-policy-iteration.png\" width=600 />\n",
    "\n",
    "### Principle of Optimality\n",
    "Any optimal policy can be subdivided into two components:\n",
    "* An optimal first action $A_{∗}$\n",
    "* Followed by an optimal policy from successor state S'\n",
    "\n",
    "**Theorem (Principle of Optimality)**\n",
    "\n",
    "A policy π(a|s) achieves the optimal value from state s, $v_{π} (s) = v_{∗} (s)$, if and only if\n",
    "* For any state s' reachable from s\n",
    "* π achieves the optimal value from state s' , $v_{π} (s') = v_{∗} (s')$\n",
    "\n",
    "## 3.4 Value Iteration\n",
    "### Deterministic Value Iteration\n",
    "* If we know the solution to subproblems $v_{∗} (s)$\n",
    "* Then solution $v_{∗} (s)$ can be found by one-step lookahead\n",
    "$$ v_{∗} (s) ← \\underset{a \\in A}{max} R^{a}_{s} + \\gamma \\underset{s' \\in S}{\\sum}P^{a}_{ss'}v_{*}(s')$$\n",
    "* The idea of value iteration is to apply these updates iteratively\n",
    "* Intuition: start with final rewards and work backwards\n",
    "* **Still works with loopy, stochastic MDPs**\n",
    "\n",
    "### Example\n",
    "<img src=\"images/rl-dp-value-interation-example.png\" width=600 />\n",
    "\n",
    "### Value Iteration Algorithm\n",
    "* Problem: find optimal policy π\n",
    "* Solution: iterative application of Bellman optimality backup\n",
    "* $v_{1} → v_{2} → ... → v_{∗}$\n",
    "* Using **synchronous backups**\n",
    "    * At each iteration k + 1\n",
    "    * For all states s ∈ S\n",
    "    * Update $v_{k+1} (s)$ from $v_{k} (s')$\n",
    "* Convergence to $v_{∗}$ will be proven later\n",
    "* **Unlike policy iteration, there is no explicit policy**\n",
    "* Intermediate value functions may not correspond to any policy\n",
    "\n",
    "<img src=\"images/rl-dp-value-iteration.png\" width=600 />\n",
    "\n",
    "### policy iteration vs value iteration\n",
    "$$ \\tag{Policy Iteration} v_{k+1}(s) = \\underset{a \\in A}{\\sum}\\pi(a|s) (R^{a}_{s} + \\gamma \\underset{s' \\in S}{\\sum}(P^{a}_{ss'} v_{k}(s') )))$$\n",
    "$$ \\tag{Value  Iteration} v_{k+1}(s) = \\underset{a \\in A}{max}(R^{a}_{s} + \\gamma \\underset{s' \\in S}{\\sum}(P^{a}_{ss'} v_{k}(s') )$$\n",
    "$$ \\tag{Policy Iteration Matrix Form} v^{k+1} = R^{\\pi} + \\gamma P^{\\pi}v^{k}$$\n",
    "$$ \\tag{Value  Iteration Matrix Form} v_{k+1} = \\underset{a \\in A}{max} (R^{a} + \\gamma P^{a}v_{k})$$\n",
    "\n",
    "## Summary of DP Algorithms\n",
    "<img src=\"images/rl-dp-synchronous-dp-algs.png\" width=600 />\n",
    "\n",
    "## 3.5 Extensions to Dynamic Programming\n",
    "## 3.6 Contraction Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model-Free Prediction\n",
    "# 5 Model-Free Control\n",
    "# 6 Value Function Approximation\n",
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
