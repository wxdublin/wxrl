{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 4\n",
    "\n",
    "# 9 Exploration and Exploitation\n",
    "## 9.1 Introduction\n",
    "** Exploration vs. Exploitation Dilemma **\n",
    "* Online decision-making involves a fundamental choice:\n",
    "    * Exploitation Make the best decision given current information\n",
    "    * Exploration Gather more information\n",
    "* The best long-term strategy may involve short-term sacrifices\n",
    "* Gather enough information to make the best overall decisions\n",
    "\n",
    "** Examples **\n",
    "* Restaurant Selection\n",
    "    * Exploitation Go to your favourite restaurant\n",
    "    * Exploration Try a new restaurant\n",
    "* Online Banner Advertisements\n",
    "    * Exploitation Show the most successful advert\n",
    "    * Exploration Show a different advert\n",
    "* Oil Drilling\n",
    "    * Exploitation Drill at the best known location\n",
    "    * Exploration Drill at a new location\n",
    "* Game Playing\n",
    "    * Exploitation Play the move you believe is best\n",
    "    * Exploration Play an experimental move\n",
    "\n",
    "** Principles **\n",
    "* Naive Exploration\n",
    "    * Add noise to greedy policy (e.g. $\\epsilon$-greedy)\n",
    "* Optimistic Initialisation\n",
    "    * Assume the best until proven otherwise\n",
    "* Optimism in the Face of Uncertainty\n",
    "    * Prefer actions with uncertain values\n",
    "* Probability Matching\n",
    "    * Select actions according to probability they are best\n",
    "* Information State Search\n",
    "    * Lookahead search incorporating value of information\n",
    "    \n",
    "## 9.2 Multi-Armed Bandits\n",
    "** The Multi-Armed Bandit **\n",
    "* A multi-armed bandit is a tuple $<A, R>$\n",
    "* A is a known set of m actions (or “arms”)\n",
    "* $R^a (r ) = P [r |a]$ is an unknown probability distribution over rewards\n",
    "* At each step t the agent selects an action $a_t ∈ A$\n",
    "* The environment generates a reward $r_t ∼ R^{a_t}$\n",
    "* The goal is to maximise cumulative reward $\\sum^t_{\\tau = 1}r_{\\tau}$\n",
    "\n",
    "### 9.2.1 Regret\n",
    "** Regret **\n",
    "* The action-value is the mean reward for action a, $$Q(a) = \\mathbb{E} [r |a]$$\n",
    "* The optimal value $V^∗$ is $$V^∗ = Q(a^∗ ) = \\underset{a \\in A}{max} Q(a) $$\n",
    "* The regret is the opportunity loss for one step $$l_t = E [V^∗ − Q(a_t )]$$\n",
    "* The total regret is the total opportunity loss \n",
    "$$L_t = \\mathbb{E} \\left[ \\sum_{\\tau = 1}^{t} V^* - Q(a_{\\tau}) \\right]$$\n",
    "* Maximise cumulative reward ≡ minimise total regret\n",
    "\n",
    "** Counting regret** \n",
    "<img width=600 src=\"images/rl-xx-counting-regrets.png\" />\n",
    "\n",
    "** Linear and Sub-linear Regret**\n",
    "<img width=600 src=\"images/rl-xx-sublinear-regret.png\" />\n",
    "\n",
    "### 9.2.2 Greedy and $\\epsilon$-greedy algorithms\n",
    "\n",
    "** Greedy Algorithm **\n",
    "* We consider algorithms that estimate $\\hat{Q}_t (a) ≈ Q(a)$\n",
    "* Estimate the value of each action by Monte-Carlo evaluation\n",
    "$$ \\hat{Q}_t (a) = \\frac{1}{N_t(a)}\\sum^T_{t=1}r_t \\mathbf{1}(a_t =a)$$\n",
    "* The greedy algorithm selects action with highest value\n",
    "$$a_t^∗ = \\underset{a \\in A}{argmax} \\hat{Q}_t(a)$$\n",
    "* Greedy can lock onto a suboptimal action forever\n",
    "* ⇒ Greedy has linear total regret\n",
    "\n",
    "** $\\epsilon$-Greedy Algorithm **\n",
    "* The $\\epsilon$-greedy algorithm continues to explore forever\n",
    "    * With probability 1 − $\\epsilon$ select $a = \\underset{a \\in A}{argmax} \\hat{Q}(a)$\n",
    "    * With probability $\\epsilon$ select a random action\n",
    "* Constant $\\epsilon$ ensures minimum regret\n",
    "$$l_t ≥ \\frac{\\epsilon}{A}\\sum_{a \\in A}\\Delta a$$\n",
    "* ⇒ -greedy has linear total regret\n",
    "\n",
    "** Optimistic Initialisation **\n",
    "* Simple and practical idea: initialise Q(a) to high value\n",
    "* Update action value by incremental Monte-Carlo evaluation\n",
    "* Starting with N(a) > 0\n",
    "$$ \\hat{Q}_t (a_t ) = \\hat{Q}_{t-1} + \\frac{1}{N_t(a_t)}(r(t) -\\hat{Q}_{t-1)}$$\n",
    "* Encourages systematic exploration early on\n",
    "* But can still lock onto suboptimal action\n",
    "* ⇒ greedy + optimistic initialisation has linear total regret\n",
    "* ⇒ $\\epsilon$-greedy + optimistic initialisation has linear total regret\n",
    "\n",
    "** Decaying $\\epsilon_t$-Greedy Algorithm **\n",
    "* Pick a decay schedule for $\\epsilon_1 , \\epsilon_2 $, ...\n",
    "* Consider the following schedule\n",
    "$$ c > 0 \\\\\n",
    "d = \\underset{a \\mid \\Delta a > 0}{min} \\Delta i \\\\\n",
    "\\epsilon_t  = min \\left \\{ 1, \\frac{c|A|}{d^2 t} \\right \\} $$\n",
    "* has logarithmic asymptotic total regret!\n",
    "* Unfortunately, schedule requires advance knowledge of gaps\n",
    "* Goal: find an algorithm with sublinear regret for any multi-armed bandit (without knowledge of R)\n",
    "\n",
    "### 9.2.3 Upper Confidence Bound\n",
    "** Lower Bound **\n",
    "* The performance of any algorithm is determined by similarity between optimal arm and other arms\n",
    "* Hard problems have similar-looking arms with different means\n",
    "* This is described formally by the gap $∆_a$ and the similarity in distributions $KL(R^a ||R^{a_∗})$\n",
    "\n",
    "** Theorem (Lai and Robbins) **\n",
    "\n",
    "Asymptotic total regret is at least logarithmic in number of steps\n",
    "$$\\underset{t \\to \\infty}{lim} L_t ≥ log t \\sum_{a|∆_a >0} \\frac{∆_a}{KL(R^a ||R^{a^∗})} $$\n",
    "\n",
    "<img width=600 src=\"images/rl-xx-optimism-uncertainty.png\" />\n",
    "<img width=600 src=\"images/rl-xx-optimism-uncertainty2.png\" />\n",
    "\n",
    "**Upper Confidence Bound**\n",
    "\n",
    "* Estimate an upper confidence $\\hat{U}_t (a)$ for each action value\n",
    "* Such that $Q(a) \\leq \\hat{Q}_t(a)+ \\hat{U}_t (a)$ with high probability\n",
    "* This depends on the number of times N(a) has been selected\n",
    "    * Small $N_t (a) \\implies large \\hat{U}_t(a)$ (estimated value is uncertain)\n",
    "    * Large $N_t (a) \\implies small \\hat{U}_t(a)$ (estimated value is accurate)\n",
    "* Select action maximising **Upper Confidence Bound (UCB)**\n",
    "$$ a_t = \\underset{a \\in A}{argmax}\\hat{Q}_t (a) + \\hat{U}_t (a) $$\n",
    "\n",
    "** Hoeffding’s Inequality **\n",
    "Theorem (Hoeffding’s Inequality)\n",
    "Let $X_1 , ..., X_t$ be i.i.d. random variables in [0,1], and let $\\overline{X}_t = \\frac{1}{t}\\sum_{\\tau=1}^t X_{\\tau}$ be the **sample mean**. Then\n",
    "$$\\mathbb{P} [ \\mathbb{E} [X] > \\overline{X}_t + u] ≤ e^{−2tu^2}$$\n",
    "\n",
    "* We will apply Hoeffding’s Inequality to rewards of the bandit\n",
    "* conditioned on selecting action a\n",
    "$$\\mathbb{P} [Q(a) > \\hat{Q}_t (a) + U_t (a)] \\leq e^{ −2N_t(a)U_t(a)^2}$$\n",
    "\n",
    "** Calculating Upper Confidence Bounds **\n",
    "* Pick a probability p that true value exceeds UCB\n",
    "* Now solve for $U_t (a)$\n",
    "$$ e^{−2N_t(a)U_t(a)^2} = p \\\\\n",
    "U_t (a) = \\sqrt {\\frac {− log p} {2N_t (a)}}$$\n",
    "* Reduce p as we observe more rewards, e.g. $p = t^{−4}$\n",
    "* Ensures we select optimal action as $t \\to \\infty$ \n",
    "$$ U_t (a) = \\sqrt {\\frac {2 log t}{N_t (a)}} $$\n",
    "\n",
    "**UCB1**\n",
    "\n",
    "This leads to the UCB1 algorithm\n",
    "$$a_t = \\underset{a \\in A}{argmax} Q(a) + \\sqrt{\\frac{2 log t}{N_t (a)}} $$\n",
    "\n",
    "**Theorem** The UCB algorithm achieves logarithmic asymptotic total regret\n",
    "$$ \\underset{t \\to \\infty}{lim} L_t \\leq 8 log t \\sum_{a \\mid \\Delta a > 0}\\Delta a$$\n",
    "\n",
    "### 9.2.4 Bayesian Bandits\n",
    "** Bayesian Bandits **\n",
    "* So far we have made no assumptions about the reward distribution R\n",
    "    * Except bounds on rewards\n",
    "* Bayesian bandits exploit prior knowledge of rewards, p[R]\n",
    "* They compute posterior distribution of rewards $p [R | h_t ]$\n",
    "    * where $h_t = a_1 , r_1 , ..., a_{t−1} , r_{t−1}$ is the history\n",
    "* Use posterior to guide exploration\n",
    "    * Upper confidence bounds (Bayesian UCB)\n",
    "    * Probability matching (Thompson sampling)\n",
    "* Better performance if prior knowledge is accurate\n",
    "\n",
    "<img width=600 src=\"images/rl-xx-bayesian-independent-gaussian.png\" />\n",
    "\n",
    "** Probability Matching **\n",
    "* Probability matching selects action a according to probability that a is the optimal action\n",
    "$$ π(a | h_t ) = \\mathbb{P} [Q(a) > \\hat{Q}(a ), \\forall a' \\neq a \\mid h_t ]$$\n",
    "* Probability matching is optimistic in the face of uncertainty\n",
    "    * Uncertain actions have higher probability of being max\n",
    "* Can be difficult to compute analytically from posterior\n",
    "\n",
    "** Thompson Sampling **\n",
    "* Thompson sampling implements probability matching\n",
    "$$ \\begin{align*} \\\\\n",
    "\\pi(a | h_t ) &= \\mathbb{P} [Q(a) > Q(a' ), \\forall a' \\neq a | h_t] \\\\\n",
    "&= \\mathbb{E}_{R|h_t} \\mathbf{1}(a = \\underset{a \\in A}{argmax}) Q(a) \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "* Use Bayes law to compute posterior distribution $p [R | h_t ]$\n",
    "* Sample a reward distribution R from posterior\n",
    "* Compute action-value function $Q(a) = \\mathbb{E} [R_a ]$\n",
    "* Select action maximising value on sample, $a_t = \\underset{a \\in A}{argmax} Q(a)$\n",
    "* **Thompson sampling achieves Lai and Robbins lower bound!**\n",
    "\n",
    "### 9.2.5 Information State Search\n",
    "** Value of Information **\n",
    "* Exploration is useful because it gains information\n",
    "* Can we quantify the value of information?\n",
    "    * How much reward a decision-maker would be prepared to pay in order to have that information, prior to making a decision\n",
    "    * Long-term reward after getting information - immediate reward\n",
    "* Information gain is higher in uncertain situations\n",
    "* Therefore it makes sense to explore uncertain situations more\n",
    "* **If we know value of information, we can trade-off exploration and exploitation optimally**\n",
    "\n",
    "<img width=600 src=\"images/rl-xx-information-space.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-xx-bernoulli-bandit.png\" />\n",
    "\n",
    "** Solving Information State Space Bandits **\n",
    "* We now have an infinite MDP over information states\n",
    "* This MDP can be solved by reinforcement learning\n",
    "* Model-free reinforcement learning\n",
    "    * e.g. Q-learning (Duff, 1994)\n",
    "* Bayesian model-based reinforcement learning\n",
    "    * e.g. Gittins indices (Gittins, 1979)\n",
    "    * This approach is known as Bayes-adaptive RL\n",
    "    * Finds Bayes-optimal exploration/exploitation trade-off with respect to prior distribution\n",
    "\n",
    "<img width=600 src=\"images/rl-xx-bayes-adaptive-bernoulli-bandits.png\" />\n",
    "<img width=600 src=\"images/rl-xx-bayes-mdp.png\" />\n",
    "\n",
    "** Gittins Indices for Bernoulli Bandits **\n",
    "* Bayes-adaptive MDP can be solved by dynamic programming\n",
    "* The solution is known as the **Gittins index**\n",
    "* Exact solution to Bayes-adaptive MDP is typically intractable\n",
    "    * Information state space is too large\n",
    "* Recent idea: apply simulation-based search (Guez et al. 2012)\n",
    "    * Forward search in information state space\n",
    "    * Using simulations from current information state\n",
    "\n",
    "## 9.3 Contextual Bandits\n",
    "<img width=600 src=\"images/rl-xx-contextual-bandits.png\" />\n",
    "<img width=600 src=\"images/rl-xx-linear-regression.png\" />\n",
    "<img width=600 src=\"images/rl-xx-linear-ucb.png\" />\n",
    "<img width=600 src=\"images/rl-xx-linear-ucb-intuition.png\" />\n",
    "<img width=600 src=\"images/rl-xx-linear-ucb-calculation.png\" />\n",
    "\n",
    "## 9.4 MDPs\n",
    "** Exploration/Exploitation Principles to MDPs **\n",
    "* The same principles for exploration/exploitation apply to MDPs\n",
    "* Naive Exploration\n",
    "* Optimistic Initialisation\n",
    "* Optimism in the Face of Uncertainty\n",
    "* Probability Matching\n",
    "* Information State Search\n",
    "\n",
    "### 9.4.1 Optimistic Initialisation: Model-Free RL\n",
    "* Initialise action-value function Q(s, a) to $\\frac {r_{max}}{1-\\gamma}$\n",
    "* Run favourite model-free RL algorithm\n",
    "    * Monte-Carlo control\n",
    "    * Sarsa\n",
    "    * Q-learning\n",
    "    * .\n",
    "* courages systematic exploration of states and actions\n",
    "\n",
    "### 9.4.2 Optimistic Initialisation: Model-Based RL\n",
    "* Construct an **optimistic** model of the MDP\n",
    "* Initialise transitions to **go to heaven**\n",
    "    * (i.e. transition to terminal state with $r_{max}$ reward)\n",
    "* Solve optimistic MDP by favourite planning algorithm\n",
    "    * policy iteration\n",
    "    * value iteration\n",
    "    * tree search\n",
    "    * ...\n",
    "* Encourages systematic exploration of states and actions\n",
    "    * e.g. RMax algorithm (Brafman and Tennenholtz)\n",
    "    \n",
    "### 9.4.3 Upper Confidence Bounds: Model-Free RL\n",
    "* Maximise UCB on action-value function $Q^π (s, a)$\n",
    "$$ a_t = \\underset{a \\in A}{argmax} Q(s_t , a) + U(s_t , a)$$\n",
    "    * Estimate uncertainty in policy evaluation (easy)\n",
    "    * Ignores uncertainty from policy improvement\n",
    "* Maximise UCB on optimal action-value function $Q^∗ (s, a)$\n",
    "$$ a_t = \\underset{a \\in A}{argmax} Q(s _t , a) + U_1 (s_t , a) + U_2 (s_t , a)$$\n",
    "    * Estimate uncertainty in policy evaluation (easy)\n",
    "    * plus uncertainty from policy improvement (hard)\n",
    "\n",
    "### 9.4.4 Bayesian Model-Based RL\n",
    "* Maintain posterior distribution over MDP models\n",
    "* Estimate both transitions and rewards, $p [\\mathcal{P}, \\mathcal{R} | h_t ]$\n",
    "    * where $h_t = s_1 , a_1 , r_2 , ..., s_t$ is the history\n",
    "* Use posterior to guide exploration\n",
    "* Upper confidence bounds (Bayesian UCB)\n",
    "* Probability matching (Thompson sampling)\n",
    "\n",
    "### 9.4.5 Thompson Sampling: Model-Based RL\n",
    "* Thompson sampling implements probability matching\n",
    "$$\\begin{align*} \\\\\n",
    "π(s, a | h_t ) &= \\mathbb{P} [Q^∗ (s, a) > Q^∗ (s, a' ), \\forall a \\neq a' | h_t] \\\\\n",
    "&= \\mathbb{E}_{\\mathcal{P,R}|h_t} \\left[\\mathbf{1}(a = \\underset{a \\in A}{argmax} Q^∗ (s, a))\\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "* Use Bayes law to compute posterior distribution $p [\\mathcal{P}, \\mathcal{R} | h_t ]$\n",
    "* **Sample** an MDP P, R from posterior\n",
    "* Solve MDP using favourite planning algorithm to get $Q^∗ (s, a)$\n",
    "* Select optimal action for sample MDP, $a_t = \\underset{a \\in A}{argmax} Q^∗ (s_t , a)$\n",
    "\n",
    "### 9.4.6 Information State Search in MDPs\n",
    "* MDPs can be augmented to include information state\n",
    "* Now the augmented state is $\\langle s,\\tilde{s} \\rangle$\n",
    "    * where s is original state within MDP\n",
    "    * and $\\tilde{s}$ is a statistic of the history (accumulated information)\n",
    "* Each action a causes a transition\n",
    "    * to a new state s' with probability $\\mathcal{P}^a_{s, s'}$\n",
    "    * to a new information state $\\tilde{s}'$\n",
    "* Defines MDP $\\tilde{\\mathcal{M}}$ in augmented information state space\n",
    "    $$ \\tilde{\\mathcal{M}} = \\langle \\tilde{S},A,\\tilde{P},R,\\gamma \\rangle$$\n",
    "\n",
    "### 9.4.7 Bayes Adaptive MDPs\n",
    "* Posterior distribution over MDP model is an information state\n",
    "$$ \\tilde{s}_t = \\mathbb{P} [\\mathcal{P}, \\mathcal{R} \\mid h_t]$$\n",
    "* Augmented MDP over $\\langle s, \\tilde{s} \\rangle$ is called Bayes-adaptive MDP\n",
    "* Solve this MDP to find optimal exploration/exploitation trade-off (with respect to prior)\n",
    "* However, Bayes-adaptive MDP is typically enormous\n",
    "* Simulation-based search has proven effective (Guez et al.)\n",
    "\n",
    "**Conclusion**\n",
    "* Have covered several principles for exploration/exploitation\n",
    "* Naive methods such as $\\epsilon$-greedy\n",
    "* Optimistic initialisation\n",
    "* Upper confidence bounds\n",
    "* Probability matching\n",
    "* Information state search\n",
    "* Each principle was developed in bandit setting\n",
    "* But same principles also apply to MDP setting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Case study - RL in games\n",
    "## 10.1 State of the Art\n",
    "** Why Study Classic Games? **\n",
    "* Simple rules, deep concepts\n",
    "* Studied for hundreds or thousands of years\n",
    "* Meaningful IQ test\n",
    "* Drosophila of artificial intelligence\n",
    "* Microcosms encapsulating real world issues\n",
    "* Games are fun!\n",
    "<img width=600 src=\"images/rl-game-ai-in-games.png\" />\n",
    "<img width=600 src=\"images/rl-game-ai-in-games2.png\" />\n",
    "\n",
    "## 10.2 Game Theory\n",
    "** Optimality in Games **\n",
    "* What is the optimal policy $π^i$ for ith player?\n",
    "* If all other players fix their policies $π^{−i}$\n",
    "* **Best response** $π^i_∗ (π^{−i} )$ is optimal policy against those policies\n",
    "* **Nash equilibrium** is a joint policy for all players\n",
    "$$π^i = π_∗^i (π^{−i} )$$\n",
    "* such that every player’s policy is a best response\n",
    "* i.e. no player would choose to deviate from Nash\n",
    "\n",
    "** Single-Agent and Self-Play Reinforcement Learning **\n",
    "* **Best response** is **solution** to single-agent RL problem\n",
    "    * Other players become part of the environment\n",
    "    * Game is reduced to an MDP\n",
    "    * Best response is optimal policy for this MDP\n",
    "* **Nash equilibrium** is **fixed-point** of self-play RL\n",
    "    * Experience is generated by playing games between agents\n",
    "    $$a_1 ∼ π^1 , a_2 ∼ π^2 , ...$$\n",
    "    * Each agent learns best response to other players\n",
    "    * One player’s policy determines another player’s environment\n",
    "    * All players are adapting to each other\n",
    "\n",
    "** Two-Player Zero-Sum Games **\n",
    "* We will focus on a special class of games:\n",
    "    * A two-player game has two (alternating) players\n",
    "        * We will name player 1 white and player 2 black\n",
    "    * A zero sum game has equal and opposite rewards for black and white\n",
    "    $$R^1 + R^2 = 0$$\n",
    "* We consider methods for finding Nash equilibria in these games\n",
    "    * Game tree search (i.e. planning)\n",
    "    * Self-play reinforcement learning\n",
    "\n",
    "** Perfect and Imperfect Information Games **\n",
    "* A perfect information or Markov game is fully observed\n",
    "    * Chess\n",
    "    * Checkers\n",
    "    * Othello\n",
    "    * Backgammon\n",
    "    * Go\n",
    "* An imperfect information game is partially observed\n",
    "    * Scrabble\n",
    "    * Poker\n",
    "* We focus first on perfect information games\n",
    "\n",
    "## 10.3 Minimax Search\n",
    "\n",
    "** Minimax **\n",
    "* A value function defines the expected total reward given joint policies $π = \\langle π^1 , π^2 \\rangle$\n",
    "$$v_π (s) = \\mathbb{E}_π [G_t \\mid S_t = s]$$\n",
    "* A minimax value function maximizes white’s expected return while minimizing black’s expected return\n",
    "$$v_∗ (s) = \\underset{\\pi ^ 1}{max} \\underset{\\pi ^ 2}{min} v_π (s)$$\n",
    "* A minimax policy is a joint policy $π = \\langle π^1 , π^2 \\rangle$ that achieves the minimax values\n",
    "* There is a unique minimax value function\n",
    "* A minimax policy is a Nash equilibrium\n",
    "\n",
    "<img width=600 src=\"images/rl-game-minimax-search.png\" />\n",
    "<img width=600 src=\"images/rl-game-minimax-search2.png\" />\n",
    "<img width=600 src=\"images/rl-game-minimax-search3.png\" />\n",
    "<img width=600 src=\"images/rl-game-minimax-search4.png\" />\n",
    "<img width=600 src=\"images/rl-game-minimax-search5.png\" />\n",
    "\n",
    "** Value Function in Minimax Search **\n",
    "* Search tree grows exponentially\n",
    "* Impractical to search to the end of the game\n",
    "* Instead use value function approximator $v(s, w) ≈ v_∗ (s)$\n",
    "    * aka **evaluation function**, **heuristic function**\n",
    "* Use value function to estimate minimax value at leaf nodes\n",
    "* Minimax search run to fixed depth with respect to leaf values\n",
    "\n",
    "<img width=600 src=\"images/rl-game-binary-linear-value-function.png\" />\n",
    "<img width=600 src=\"images/rl-game-deep-blue.png\" />\n",
    "<img width=600 src=\"images/rl-game-chinook.png\" />\n",
    "\n",
    "## 10.4 Self-Play Reinforcement Learning\n",
    "<img width=600 src=\"images/rl-game-self-play-td-learning.png\" />\n",
    "<img width=600 src=\"images/rl-game-policy-improvement-with-afterstates.png\" />\n",
    "<img width=600 src=\"images/rl-game-logistello.png\" />\n",
    "<img width=600 src=\"images/rl-game-reinforcement-learning-in-logistello.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-gammon-non-linear.png\" />\n",
    "<img width=600 src=\"images/rl-game-self-play-td-in-td-gammon.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-gammon-results.png\" />\n",
    "\n",
    "## 10.5 Combining Reinforcement Learning and Minimax Search\n",
    "<img width=600 src=\"images/rl-game-simple-td.png\" />\n",
    "<img width=600 src=\"images/rl-game-simple-td-results.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-root.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-root-in-checker.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-leaf.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-leaf-in-chess.png\" />\n",
    "<img width=600 src=\"images/rl-game-td-leaf-in-checker.png\" />\n",
    "<img width=600 src=\"images/rl-game-treestrap.png\" />\n",
    "<img width=600 src=\"images/rl-game-treestrap-in-chess.png\" />\n",
    "<img width=600 src=\"images/rl-game-simulation-based-search.png\" />\n",
    "<img width=600 src=\"images/rl-game-performance-of-mcts.png\" />\n",
    "<img width=600 src=\"images/rl-game-simple-mcts-in-maven.png\" />\n",
    "\n",
    "## 10.6 Reinforcement Learning in Imperfect-Information Games\n",
    "<img width=600 src=\"images/rl-game-imperfect-info.png\" />\n",
    "<img width=600 src=\"images/rl-game-imperfect-info-solution.png\" />\n",
    "<img width=600 src=\"images/rl-game-smooth-uct-search.png\" />\n",
    "\n",
    "## 10.7 Summary\n",
    "<img width=600 src=\"images/rl-game-summary.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
