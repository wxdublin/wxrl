{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 2\n",
    "\n",
    "# 4 Model-Free Prediction\n",
    "\n",
    "## 4.1 Introduction\n",
    "\n",
    "**Planning by dynamic programming**\n",
    "\n",
    "* Solve a known MDP\n",
    "\n",
    "**Model-free prediction**\n",
    "\n",
    "* Estimate the value function of an unknown MDP\n",
    "    * MC: Monte-Carlo\n",
    "    * TD: Time Difference\n",
    "    \n",
    "**Model-free control**\n",
    "\n",
    "* Optimise the value function of an unknown MDP\n",
    "\n",
    "## 4.2 Monte-Carlo Learning\n",
    "* MC methods learn directly from **episodes** of experience\n",
    "* MC is **model-free**: no knowledge of MDP transitions / rewards\n",
    "* MC learns from complete episodes: **no bootstrapping**\n",
    "* MC uses the simplest possible idea: **value = mean return**\n",
    "* Caveat: can only apply MC to episodic MDPs\n",
    "* **All episodes must terminate**\n",
    "\n",
    "### 4.2.1 Monte-Carlo Policy Evaluation\n",
    "* Goal: learn $v_π$ from episodes of experience under policy π\n",
    "$$ S_1, A_1, R_2, ..., S_k \\sim \\pi $$\n",
    "* Recall that the return is the total discounted reward:\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1}R_T$$\n",
    "* Recall that the value function is the **expected return**:\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s]$$\n",
    "* Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return\n",
    "\n",
    "**First-Visit Monte-Carlo Policy Evaluation**\n",
    "* To evaluate state s\n",
    "* The first time-step t that state s is visited in an episode,\n",
    "* Increment counter $N(s) ← N(s) + 1$\n",
    "* Increment total return $S(s) ← S(s) + G_t$\n",
    "* Value is estimated by mean return $V(s) = S(s)/N(s)$\n",
    "* By law of large numbers, $V (s) → v_{\\pi}(s)$ as $N(s) → \\infty$\n",
    "\n",
    "**Every-Visit Monte-Carlo Policy Evaluation**\n",
    "* To evaluate state s\n",
    "* Every time-step t that state s is visited in an episode,\n",
    "* Increment counter $N(s) ← N(s) + 1$\n",
    "* Increment total return $S(s) ← S(s) + G_t$\n",
    "* Value is estimated by mean return $V (s) = S(s)/N(s)$\n",
    "* Again, $V (s) → v_{\\pi} (s)$ as $N(s) → \\infty$\n",
    "\n",
    "**Blackjack Example**\n",
    "* States (200 of them):\n",
    "    * Current sum (12-21)\n",
    "    * Dealer’s showing card (ace-10)\n",
    "    * Do I have a “useable” ace? (yes-no)\n",
    "* Action stick: Stop receiving cards (and terminate)\n",
    "* Action twist: Take another card (no replacement)\n",
    "* Reward for stick:\n",
    "    * +1 if sum of cards > sum of dealer cards\n",
    "    * 0 if sum of cards = sum of dealer cards\n",
    "    * -1 if sum of cards < sum of dealer cards\n",
    "* Reward for twist:\n",
    "    * -1 if sum of cards > 21 (and terminate)\n",
    "    * 0 otherwise\n",
    "* Transitions: automatically twist if sum of cards < 12\n",
    "\n",
    "**Incremental Mean**\n",
    "\n",
    "The mean $μ_1 , μ_2 , ...$ of a sequence $x_1 , x_2 , ...$ can be computed incrementally,\n",
    "$$ \\begin{align*} \\\\\n",
    "μ_k &= \\frac{1}{k} \\sum^k_{j=1} x_j \\\\\n",
    "&= \\frac{1}{k} (x_k + \\sum^{k-1}_{j=1} x_j) \\\\\n",
    "&= \\frac{1}{k} (x_k + (k-1)μ_{k-1}) \\\\\n",
    "&= μ_{k-1} + \\frac{1}{k} (x_k - μ_{k-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Incremental Monte-Carlo Updates**\n",
    "* Update V(s) incrementally after episode $S_1 , A_1 , R_2 , ..., S_T$\n",
    "* For each state $S_t$ with return $G_t$\n",
    "$$ N(S_t) \\gets N(S_t) + 1 \\\\\n",
    "V(S_t) \\gets V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t)$$\n",
    "* In **non-stationary problems**, it can be useful to track a **running mean**, i.e. forget old episodes.\n",
    "$$V(S_t) \\gets V(S_t) + \\alpha (G_t - V(S_t)$$\n",
    "\n",
    "\n",
    "## 4.3 Temporal-Difference Learning\n",
    "* TD methods learn directly from episodes of experience\n",
    "* TD is model-free: no knowledge of MDP transitions / rewards\n",
    "* TD learns from **incomplete episodes, by bootstrapping**\n",
    "* **TD updates a guess towards a guess**\n",
    "\n",
    "** MC and TD **\n",
    "\n",
    "* Goal: learn $v_π$ online from experience under policy π\n",
    "* Incremental every-visit Monte-Carlo\n",
    "    * Update value $V (S_t )$ toward actual return $G_t$\n",
    "$$ V (S_t ) \\gets V (S_t ) + α (G_t − V (S_t ))$$\n",
    "* Simplest temporal-difference learning algorithm: **TD(0)**\n",
    "    * Update value $V (S_t )$ toward estimated return $R_{t+1} + \\gamma V (S_{t+1} )$\n",
    "$$ V (S_t ) \\gets V (S_t ) + \\alpha (R_{t+1} + \\gamma V (S_{t+1} ) − V (S_t ))$$\n",
    "    * $R_{t+1} + \\gamma V (S_{t+1} )$ is called the **TD target**\n",
    "    * $\\delta_t = R_{t+1} + \\gamma V (S_{t+1} ) − V (S_t )$ is called the **TD error**\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD **\n",
    "\n",
    "* **TD can learn before knowing the final outcome**\n",
    "    * TD can learn online after every step\n",
    "    * MC must wait until end of episode before return is known\n",
    "* **TD can learn without the final outcome**\n",
    "    * TD can learn from incomplete sequences\n",
    "    * MC can only learn from complete sequences\n",
    "    * **TD works in continuing (non-terminating) environments**\n",
    "    * MC only works for episodic (terminating) environments\n",
    "\n",
    "** Bias/Variance Trade-Off **\n",
    "* Return $G_t = R_{t+1} + γR_{t+2} + ... + γ^{T −1} R_T$ is unbiased estimate of $v_π (S_t )$\n",
    "* True TD target $R_{t+1} + γv_π (S_{t+1} )$ is unbiased estimate of $v_π (S_t )$\n",
    "* TD target $R_{t+1} + γV (S_{t+1} )$ is **biased estimate** of v π (S t )\n",
    "* TD target is much lower variance than the return:\n",
    "    * Return depends on many random actions, transitions, rewards\n",
    "    * TD target depends on one random action, transition, reward\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD (2) **\n",
    "* **MC has high variance, zero bias**\n",
    "    * Good convergence properties\n",
    "    * (even with function approximation)\n",
    "    * Not very sensitive to initial value\n",
    "    * Very simple to understand and use\n",
    "* **TD has low variance, some bias**\n",
    "    * **Usually more efficient than MC**\n",
    "    * TD(0) converges to $v_π (s)$\n",
    "    * **(but not always with function approximation)**\n",
    "    * More sensitive to initial value\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD (3) **\n",
    "* TD exploits Markov property\n",
    "    * Usually **more efficient in Markov environments**\n",
    "* MC does not exploit Markov property\n",
    "    * Usually **more effective in non-Markov environments**\n",
    "    \n",
    "** Certainty Equivalency **\n",
    "<img src=\"images/rl-mc-td-certainty-equivalence.png\" width=600 />\n",
    "\n",
    "** mc vs td vs dp backup **\n",
    "<img src=\"images/rl-mc-backup.png\" width=600 />\n",
    "<img src=\"images/rl-td-backup.png\" width=600 />\n",
    "<img src=\"images/rl-dp-backup.png\" width=600 />\n",
    "\n",
    "** Bootstrapping and Sampling **\n",
    "\n",
    "* Bootstrapping: update involves an estimate\n",
    "    * MC does not bootstrap\n",
    "    * DP bootstraps\n",
    "    * TD bootstraps\n",
    "\n",
    "* Sampling: update samples an expectation\n",
    "    * MC samples\n",
    "    * DP does not sample\n",
    "    * TD samples\n",
    "\n",
    "## 4.4 TD(λ)\n",
    "### 4.4.1 Unified View of Reinforcement Learning\n",
    "<img src=\"images/rl-unified-view.png\" width=600 />\n",
    "\n",
    "### 4.4.2 n-step prediction and return \n",
    "<img src=\"images/rl-td-nstep-prediction.png\" width=600 />\n",
    "<img src=\"images/rl-td-nstep-return.png\" width=600 />\n",
    "\n",
    "### 4.4.3 Average and $TD(\\lambda)$ \n",
    "<img src=\"images/rl-td-nstep-return-average.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-return.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-weighting.png\" width=600 />\n",
    "\n",
    "### 4.4.4 forward view and backward view\n",
    "<img src=\"images/rl-td-lambda-forward-view.png\" width=600 />\n",
    "\n",
    "* **Forward view provides theory**\n",
    "* ** Backward view provides mechanism**\n",
    "* Update online, every step, from incomplete sequences\n",
    "\n",
    "<img src=\"images/rl-td-eligibility-trace.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-backward-view2.png\" width=600 />\n",
    "\n",
    "#### Backward View $TD(\\lambda)$ Formula\n",
    "$$ E_0(s) = 0 $$\n",
    "$$ \\tag{Eligibility Trace} E_t(s) = \\gamma \\lambda E_{t-1}(s) + 1(S_t = s) $$\n",
    "$$ \\tag{delta} \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) $$\n",
    "$$ \\tag{mean} V(s) \\gets V(s) + \\alpha \\delta_t E_t(s)$$\n",
    "\n",
    "### 4.4.5 TD(λ) and TD(0)\n",
    "* When λ = 0, only current state is updated\n",
    "$$E_t (s) = 1(S_t = s) \\\\\n",
    "V(s) \\gets V(s) + \\alpha \\delta_t E_t (s)$$\n",
    "* This is exactly equivalent to TD(0) update\n",
    "$$ V (S_t ) \\gets V (S_t ) + \\alpha \\delta_t$$\n",
    "\n",
    "### 4.4.6 TD(λ) and MC\n",
    "* When λ = 1, credit is deferred until end of episode\n",
    "* Consider episodic environments with offline updates\n",
    "* Over the course of an episode, total update for TD(1) is the same as total update for MC\n",
    "\n",
    "**Theorem**\n",
    "The sum of offline updates is identical for forward-view and backward-view TD(λ)\n",
    "$$ \\sum_{t=1}^T \\alpha \\delta_t E_t(s) = \\sum_{t=1}^T \\alpha (G^{\\lambda}_t - V(S_t)) 1(S_t=s)$$\n",
    "<img src=\"images/rl-mc-td1.png\" width=600 />\n",
    "<img src=\"images/rl-mc-td2.png\" width=600 />\n",
    "\n",
    "* TD(1) is roughly equivalent to every-visit Monte-Carlo\n",
    "* Error is accumulated online, step-by-step\n",
    "* If value function is only updated offline at end of episode\n",
    "* Then total update is exactly the same as MC\n",
    "\n",
    "<img src=\"images/rl-td-telescoping.png\" width=600 />\n",
    "<img src=\"images/rl-td-error.png\" width=600 />\n",
    "\n",
    "** Offline updates **\n",
    "* Updates are accumulated within episode\n",
    "* but applied in batch at the end of episode\n",
    "\n",
    "** Online updates **\n",
    "* TD(λ) updates are applied online at each step within episode\n",
    "* Forward and backward-view TD(λ) are slightly different\n",
    "* NEW: **Exact online TD(λ)** achieves perfect equivalence\n",
    "* By using a slightly different form of eligibility trace\n",
    "* Sutton and von Seijen, ICML 2014\n",
    "\n",
    "### 4.4.7 Summary\n",
    "<img src=\"images/rl-td-summary.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model-Free Control\n",
    "> Optimise the value function of an unknown MDP\n",
    "\n",
    "## 5.1 Introduction\n",
    "### Uses of Model-Free Control\n",
    "Some example problems that can be modelled as MDPs\n",
    "* Elevator \n",
    "* Robocup Soccer\n",
    "* Parallel Parking \n",
    "* Quake\n",
    "* Ship Steering \n",
    "* Portfolio management\n",
    "* Bioreactor \n",
    "* Protein Folding\n",
    "* Helicopter \n",
    "* Robot walking\n",
    "* Aeroplane Logistics \n",
    "* Game of Go\n",
    "For most of these problems, either:\n",
    "* **MDP model is unknown, but experience can be sampled**\n",
    "* **MDP model is known, but is too big to use, except by samples**\n",
    "\n",
    "<font color=red> Model-free control can solve these problems </font>\n",
    "\n",
    "### On and Off-Policy Learning\n",
    "* On-policy learning\n",
    "    * “Learn on the job”\n",
    "    * Learn about policy π from experience sampled from π\n",
    "\n",
    "* Off-policy learning\n",
    "    * “Look over someone’s shoulder”\n",
    "    * Learn about policy π from experience sampled from μ\n",
    "\n",
    "## 5.2 On-Policy Monte-Carlo Control\n",
    "\n",
    "### Monte-Carlo Policy Iteration\n",
    "<img src=\"images/rl-ctrl-mc-policy-iteration.png\" width=600 />\n",
    "\n",
    "** Model-Free Policy Iteration Using Action-Value Function **\n",
    "* Greedy policy improvement over $V(s)$ requires model of MDP\n",
    "$$ \\pi'(s) = \\underset{a \\in A}{argmax}\\ R^a_s + P^a_{ss'}V(s')$$\n",
    "* Greedy policy improvement over Q(s, a) is model-free\n",
    "$$ \\pi'(s) = \\underset{a \\in A}{argmax}\\ Q(s,a)$$\n",
    "\n",
    "<img src=\"images/rl-ctrl-mc-evaluation.png\" width=600 />\n",
    "\n",
    "** ε-Greedy Policy Improvement **\n",
    "\n",
    "<img src=\"images/rl-ctrl-epsilon-greedy.png\" width=600 />\n",
    "\n",
    "<img src=\"images/rl-ctrl-mc-epsilon-greedy.png\" width=600 />\n",
    "\n",
    "### Monte-Carlo Control\n",
    "\n",
    "<img src=\"images/rl-ctrl-mc-control.png\" width=600 />\n",
    "** GLIE **\n",
    "Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "* All state-action pairs are explored infinitely many times,\n",
    "$$ \\lim_{k \\to \\infty} N_k(s,a)=\\infty $$\n",
    "* The policy converges on a greedy policy,\n",
    "$$ \\lim_{l \\to \\infty} \\pi_k (a|s) = 1(a=\\underset{a' \\in A}{argmax} Q_k(s,a′)) $$\n",
    "\n",
    "* For example, ε-greedy is GLIE if ε reduces to zero at $ε_k = \\frac{1}{k}$\n",
    "\n",
    "** GLIE Monte-Carlo Control **\n",
    "* Sample kth episode using π: ${S_1, A_1, R_2, ..., S_T } ∼ \\pi$ \n",
    "* For each state $S_t$ and action $A_t$ in the episode,\n",
    "$$  N (S_t , A_t ) \\gets N (S_t , A_t ) + 1 $$\n",
    "$$ Q(S_t,A_t) \\gets Q(S_t,A_t)+ \\frac{1}{N(S_t,A_t)} (G_t −Q(S_t,A_t))$$\n",
    "* Improve policy based on new action-value function\n",
    "$$ \\epsilon \\gets 1/k $$\n",
    "$$ \\pi ← \\epsilon-greedy(Q)$$\n",
    "\n",
    "**Theorem**\n",
    "GLIE Monte-Carlo control converges to the optimal action-value function, $Q(s, a) \\to q_∗(s, a)$\n",
    "  \n",
    "## 5.3 On-Policy Temporal-Difference Learning\n",
    "### 5.3.1 MC vs. TD Control\n",
    "* Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC)\n",
    "    * Lower variance \n",
    "    * Online\n",
    "    * Incomplete sequences\n",
    "* Natural idea: use TD instead of MC in our control loop \n",
    "    * Apply TD to Q(S, A)\n",
    "    * Use ε-greedy policy improvement Update every time-step\n",
    "    \n",
    "### 5.3.2 Sarsa\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-policy-iteration.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-algorithm.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-convergence.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-nstep.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-lambda-forward.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-lambda-backward.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-sarsa-lambda-algorithm.png\" />\n",
    "\n",
    "\n",
    "## 5.4 Off-Policy Learning\n",
    "* Evaluate target policy π(a|s) to compute $v_π(s)$ or $q_π(s,a)$ \n",
    "* While following behaviour policy μ(a|s)\n",
    "$${S_1,A_1,R_2,...,S_T} ∼ μ $$\n",
    "* Why is this important?\n",
    "    * Learn from observing humans or other agents\n",
    "    * Re-use experience generated from old policies $π_1, π_2, ..., π_{t−1}$ \n",
    "    * Learn about **optimal policy** while following **exploratory policy** \n",
    "    * Learn about multiple policies while following one policy\n",
    "    \n",
    "### Importance Sampling\n",
    "Estimate the expectation of a different distribution    \n",
    "$$ \\begin{align*} \\\\\n",
    "\\mathbb{E}_{X \\sim P}[f(X)] &= \\sum P(X)f(X) \\\\\n",
    "&= \\sum Q(X) \\frac{P(X)}{Q(X)} f(X)\\\\\n",
    "&= \\mathbb{E}_{X \\sim Q} [\\frac{P(X)}{Q(X)} f(X) ] \\\\\n",
    "\\end{align*} \\\\\n",
    "$$    \n",
    "\n",
    "### Importance Sampling for Off-Policy Monte-Carlo\n",
    "* Use returns generated from μ to evaluate π\n",
    "* Weight return $G_t$ according to similarity between policies \n",
    "* Multiply importance sampling corrections along whole episode\n",
    "$$ G^{\\pi/\\mu}_t = \\frac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)}...\\frac{\\pi(A_T|S_T)}{\\mu(A_T|S_T)} G_t $$\n",
    "* Update value towards corrected return\n",
    "$$ V(S_t) \\gets V(S_t) + \\alpha (G^{\\pi/\\mu}_t - V(S_t)) $$\n",
    "* Cannot use if μ is zero when π is non-zero\n",
    "* Importance sampling can dramatically increase variance\n",
    "\n",
    "### Importance Sampling for Off-Policy TD\n",
    "* Use TD targets generated from μ to evaluate π \n",
    "* Weight TD target R + γV (S′) by importance sampling \n",
    "* Only need a single importance sampling correction\n",
    "$$ V(S_t) \\gets V(S_t) + \\alpha (\\frac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)} (R_{t+1} + \\gamma V(S_{t+1})) - V(S_t))$$ \n",
    "* Much lower variance than Monte-Carlo importance sampling \n",
    "* Policies only need to be similar over a single step\n",
    "\n",
    "### Q-Learning\n",
    "* We now consider off-policy learning of action-values Q(s,a) \n",
    "* No importance sampling is required\n",
    "* Next action is chosen using behaviour policy $A_{t+1} \\sim μ(· \\mid S_t)$ \n",
    "* But we consider alternative successor action $A′ \\sim π(· \\mid S_t)$\n",
    "* And update $Q(S_t,A_t)$ towards value of alternative action\n",
    "$$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t)) $$\n",
    "\n",
    "### Off-Policy Control with Q-Learning\n",
    "* We now allow both **behaviour** and **target** policies to improve\n",
    "* The target policy π is greedy w.r.t. Q(s,a)\n",
    "$$ \\pi(S_{t+1}) = \\underset{a'}{argmax} Q(S_{t+1, a'}) $$\n",
    "* The behaviour policy μ is e.g. ε-greedy w.r.t. Q(s,a)\n",
    "* The Q-learning target then simplifies:\n",
    "$$ \\begin{align*} \\\\\n",
    "R_{t+1} + \\gamma Q(S_{t+1}, A') \\\\\n",
    "&= R_{t+1} + \\gamma Q(S_{t+1}, \\underset{a'}{argmax} Q(S_{t+1}, a')) \\\\\n",
    "&= R_{t+1} + \\underset{a'}{max} \\gamma Q(S_{t+1}, a') \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<img width=600 src=\"images/rl-ctrl-q-learning.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-ctrl-q-learning-algorithm.png\" />\n",
    "\n",
    "## 5.5 Summary\n",
    "\n",
    "<img width=600 src=\"images/rl-ctrl-summary1.png\" />\n",
    "<img width=600 src=\"images/rl-ctrl-summary2.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6 Value Function Approximation\n",
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
