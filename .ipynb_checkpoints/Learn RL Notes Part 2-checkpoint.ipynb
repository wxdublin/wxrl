{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 2\n",
    "\n",
    "# 4 Model-Free Prediction\n",
    "\n",
    "## 4.1 Introduction\n",
    "\n",
    "**Planning by dynamic programming**\n",
    "\n",
    "* Solve a known MDP\n",
    "\n",
    "**Model-free prediction**\n",
    "\n",
    "* Estimate the value function of an unknown MDP\n",
    "    * MC: Monte-Carlo\n",
    "    * TD: Time Difference\n",
    "    \n",
    "**Model-free control**\n",
    "\n",
    "* Optimise the value function of an unknown MDP\n",
    "\n",
    "## 4.2 Monte-Carlo Learning\n",
    "* MC methods learn directly from **episodes** of experience\n",
    "* MC is **model-free**: no knowledge of MDP transitions / rewards\n",
    "* MC learns from complete episodes: **no bootstrapping**\n",
    "* MC uses the simplest possible idea: **value = mean return**\n",
    "* Caveat: can only apply MC to episodic MDPs\n",
    "* **All episodes must terminate**\n",
    "\n",
    "### 4.2.1 Monte-Carlo Policy Evaluation\n",
    "* Goal: learn $v_π$ from episodes of experience under policy π\n",
    "$$ S_1, A_1, R_2, ..., S_k \\sim \\pi $$\n",
    "* Recall that the return is the total discounted reward:\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1}R_T$$\n",
    "* Recall that the value function is the **expected return**:\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s]$$\n",
    "* Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return\n",
    "\n",
    "**First-Visit Monte-Carlo Policy Evaluation**\n",
    "* To evaluate state s\n",
    "* The first time-step t that state s is visited in an episode,\n",
    "* Increment counter $N(s) ← N(s) + 1$\n",
    "* Increment total return $S(s) ← S(s) + G_t$\n",
    "* Value is estimated by mean return $V(s) = S(s)/N(s)$\n",
    "* By law of large numbers, $V (s) → v_{\\pi}(s)$ as $N(s) → \\infty$\n",
    "\n",
    "**Every-Visit Monte-Carlo Policy Evaluation**\n",
    "* To evaluate state s\n",
    "* Every time-step t that state s is visited in an episode,\n",
    "* Increment counter $N(s) ← N(s) + 1$\n",
    "* Increment total return $S(s) ← S(s) + G_t$\n",
    "* Value is estimated by mean return $V (s) = S(s)/N(s)$\n",
    "* Again, $V (s) → v_{\\pi} (s)$ as $N(s) → \\infty$\n",
    "\n",
    "**Blackjack Example**\n",
    "* States (200 of them):\n",
    "    * Current sum (12-21)\n",
    "    * Dealer’s showing card (ace-10)\n",
    "    * Do I have a “useable” ace? (yes-no)\n",
    "* Action stick: Stop receiving cards (and terminate)\n",
    "* Action twist: Take another card (no replacement)\n",
    "* Reward for stick:\n",
    "    * +1 if sum of cards > sum of dealer cards\n",
    "    * 0 if sum of cards = sum of dealer cards\n",
    "    * -1 if sum of cards < sum of dealer cards\n",
    "* Reward for twist:\n",
    "    * -1 if sum of cards > 21 (and terminate)\n",
    "    * 0 otherwise\n",
    "* Transitions: automatically twist if sum of cards < 12\n",
    "\n",
    "**Incremental Mean**\n",
    "\n",
    "The mean $μ_1 , μ_2 , ...$ of a sequence $x_1 , x_2 , ...$ can be computed incrementally,\n",
    "$$ \\begin{align*} \\\\\n",
    "μ_k &= \\frac{1}{k} \\sum^k_{j=1} x_j \\\\\n",
    "&= \\frac{1}{k} (x_k + \\sum^{k-1}_{j=1} x_j) \\\\\n",
    "&= \\frac{1}{k} (x_k + (k-1)μ_{k-1}) \\\\\n",
    "&= μ_{k-1} + \\frac{1}{k} (x_k - μ_{k-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Incremental Monte-Carlo Updates**\n",
    "* Update V(s) incrementally after episode $S_1 , A_1 , R_2 , ..., S_T$\n",
    "* For each state $S_t$ with return $G_t$\n",
    "$$ N(S_t) \\gets N(S_t) + 1 \\\\\n",
    "V(S_t) \\gets V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t)$$\n",
    "* In **non-stationary problems**, it can be useful to track a **running mean**, i.e. forget old episodes.\n",
    "$$V(S_t) \\gets V(S_t) + \\alpha (G_t - V(S_t)$$\n",
    "\n",
    "\n",
    "## 4.3 Temporal-Difference Learning\n",
    "* TD methods learn directly from episodes of experience\n",
    "* TD is model-free: no knowledge of MDP transitions / rewards\n",
    "* TD learns from **incomplete episodes, by bootstrapping**\n",
    "* **TD updates a guess towards a guess**\n",
    "\n",
    "** MC and TD **\n",
    "\n",
    "* Goal: learn $v_π$ online from experience under policy π\n",
    "* Incremental every-visit Monte-Carlo\n",
    "    * Update value $V (S_t )$ toward actual return $G_t$\n",
    "$$ V (S_t ) \\gets V (S_t ) + α (G_t − V (S_t ))$$\n",
    "* Simplest temporal-difference learning algorithm: **TD(0)**\n",
    "    * Update value $V (S_t )$ toward estimated return $R_{t+1} + \\gamma V (S_{t+1} )$\n",
    "$$ V (S_t ) \\gets V (S_t ) + \\alpha (R_{t+1} + \\gamma V (S_{t+1} ) − V (S_t ))$$\n",
    "    * $R_{t+1} + \\gamma V (S_{t+1} )$ is called the **TD target**\n",
    "    * $\\delta_t = R_{t+1} + \\gamma V (S_{t+1} ) − V (S_t )$ is called the **TD error**\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD **\n",
    "\n",
    "* **TD can learn before knowing the final outcome**\n",
    "    * TD can learn online after every step\n",
    "    * MC must wait until end of episode before return is known\n",
    "* **TD can learn without the final outcome**\n",
    "    * TD can learn from incomplete sequences\n",
    "    * MC can only learn from complete sequences\n",
    "    * **TD works in continuing (non-terminating) environments**\n",
    "    * MC only works for episodic (terminating) environments\n",
    "\n",
    "** Bias/Variance Trade-Off **\n",
    "* Return $G_t = R_{t+1} + γR_{t+2} + ... + γ^{T −1} R_T$ is unbiased estimate of $v_π (S_t )$\n",
    "* True TD target $R_{t+1} + γv_π (S_{t+1} )$ is unbiased estimate of $v_π (S_t )$\n",
    "* TD target $R_{t+1} + γV (S_{t+1} )$ is **biased estimate** of v π (S t )\n",
    "* TD target is much lower variance than the return:\n",
    "    * Return depends on many random actions, transitions, rewards\n",
    "    * TD target depends on one random action, transition, reward\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD (2) **\n",
    "* **MC has high variance, zero bias**\n",
    "    * Good convergence properties\n",
    "    * (even with function approximation)\n",
    "    * Not very sensitive to initial value\n",
    "    * Very simple to understand and use\n",
    "* **TD has low variance, some bias**\n",
    "    * **Usually more efficient than MC**\n",
    "    * TD(0) converges to $v_π (s)$\n",
    "    * **(but not always with function approximation)**\n",
    "    * More sensitive to initial value\n",
    "    \n",
    "** Advantages and Disadvantages of MC vs. TD (3) **\n",
    "* TD exploits Markov property\n",
    "    * Usually **more efficient in Markov environments**\n",
    "* MC does not exploit Markov property\n",
    "    * Usually **more effective in non-Markov environments**\n",
    "    \n",
    "** Certainty Equivalency **\n",
    "<img src=\"images/rl-mc-td-certainty-equivalence.png\" width=600 />\n",
    "\n",
    "** mc vs td vs dp backup **\n",
    "<img src=\"images/rl-mc-backup.png\" width=600 />\n",
    "<img src=\"images/rl-td-backup.png\" width=600 />\n",
    "<img src=\"images/rl-dp-backup.png\" width=600 />\n",
    "\n",
    "** Bootstrapping and Sampling **\n",
    "\n",
    "* Bootstrapping: update involves an estimate\n",
    "    * MC does not bootstrap\n",
    "    * DP bootstraps\n",
    "    * TD bootstraps\n",
    "\n",
    "* Sampling: update samples an expectation\n",
    "    * MC samples\n",
    "    * DP does not sample\n",
    "    * TD samples\n",
    "\n",
    "## 4.4 TD(λ)\n",
    "### 4.4.1 Unified View of Reinforcement Learning\n",
    "<img src=\"images/rl-unified-view.png\" width=600 />\n",
    "\n",
    "### 4.4.2 n-step prediction and return \n",
    "<img src=\"images/rl-td-nstep-prediction.png\" width=600 />\n",
    "<img src=\"images/rl-td-nstep-return.png\" width=600 />\n",
    "\n",
    "### 4.4.3 Average and $TD(\\lambda)$ \n",
    "<img src=\"images/rl-td-nstep-return-average.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-return.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-weighting.png\" width=600 />\n",
    "\n",
    "### 4.4.4 forward view and backward view\n",
    "<img src=\"images/rl-td-lambda-forward-view.png\" width=600 />\n",
    "\n",
    "* **Forward view provides theory**\n",
    "* ** Backward view provides mechanism**\n",
    "* Update online, every step, from incomplete sequences\n",
    "\n",
    "<img src=\"images/rl-td-eligibility-trace.png\" width=600 />\n",
    "<img src=\"images/rl-td-lambda-backward-view2.png\" width=600 />\n",
    "\n",
    "#### Backward View $TD(\\lambda)$ Formula\n",
    "$$ E_0(s) = 0 $$\n",
    "$$ \\tag{Eligibility Trace} E_t(s) = \\gamma \\lambda E_{t-1}(s) + 1(S_t = s) $$\n",
    "$$ \\tag{delta} \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) $$\n",
    "$$ \\tag{mean} V(s) \\gets V(s) + \\alpha \\delta_t E_t(s)$$\n",
    "\n",
    "### 4.4.5 TD(λ) and TD(0)\n",
    "* When λ = 0, only current state is updated\n",
    "$$E_t (s) = 1(S_t = s) \\\\\n",
    "V(s) \\gets V(s) + \\alpha \\delta_t E_t (s)$$\n",
    "* This is exactly equivalent to TD(0) update\n",
    "$$ V (S_t ) \\gets V (S_t ) + \\alpha \\delta_t$$\n",
    "\n",
    "### 4.4.6 TD(λ) and MC\n",
    "* When λ = 1, credit is deferred until end of episode\n",
    "* Consider episodic environments with offline updates\n",
    "* Over the course of an episode, total update for TD(1) is the same as total update for MC\n",
    "\n",
    "**Theorem**\n",
    "The sum of offline updates is identical for forward-view and backward-view TD(λ)\n",
    "$$ \\sum_{t=1}^T \\alpha \\delta_t E_t(s) = \\sum_{t=1}^T \\alpha (G^{\\lambda}_t - V(S_t)) 1(S_t=s)$$\n",
    "<img src=\"images/rl-mc-td1.png\" width=600 />\n",
    "<img src=\"images/rl-mc-td2.png\" width=600 />\n",
    "\n",
    "* TD(1) is roughly equivalent to every-visit Monte-Carlo\n",
    "* Error is accumulated online, step-by-step\n",
    "* If value function is only updated offline at end of episode\n",
    "* Then total update is exactly the same as MC\n",
    "\n",
    "<img src=\"images/rl-td-telescoping.png\" width=600 />\n",
    "<img src=\"images/rl-td-error.png\" width=600 />\n",
    "\n",
    "** Offline updates **\n",
    "* Updates are accumulated within episode\n",
    "* but applied in batch at the end of episode\n",
    "\n",
    "** Online updates **\n",
    "* TD(λ) updates are applied online at each step within episode\n",
    "* Forward and backward-view TD(λ) are slightly different\n",
    "* NEW: **Exact online TD(λ)** achieves perfect equivalence\n",
    "* By using a slightly different form of eligibility trace\n",
    "* Sutton and von Seijen, ICML 2014\n",
    "\n",
    "### 4.4.7 Summary\n",
    "<img src=\"images/rl-td-summary.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model-Free Control\n",
    "# 6 Value Function Approximation\n",
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
