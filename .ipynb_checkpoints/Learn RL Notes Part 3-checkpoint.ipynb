{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 3\n",
    "\n",
    "# 6 Value Function Approximation\n",
    "\n",
    "## Large-Scale Reinforcement Learning\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "* Backgammon: $10^{20}$ states \n",
    "* Computer Go: $10^{170}$ states \n",
    "* Helicopter: continuous state space\n",
    "How can we **scale up** the model-free methods for prediction and control from the last two lectures?\n",
    "\n",
    "So far we have represented value function by a **lookup table** \n",
    "* Every state s has an entry V(s)\n",
    "* Or every state-action pair s,a has an entry Q(s,a)\n",
    "\n",
    "Problem with large MDPs:\n",
    "* There are too many states and/or actions to store in memory \n",
    "* It is too slow to learn the value of each state individually\n",
    "\n",
    "## Value Function Approximation\n",
    "Solution for large MDPs:\n",
    "* Estimate value function with function approximation\n",
    "$$ \\hat{v} ( s , w ) \\approx v_π ( s ) $$\n",
    "or $$ \\hat{q} ( s , a , w ) \\approx q_π ( s , a )\n",
    "$$\n",
    "* Generalise from seen states to unseen states \n",
    "* Update parameter w using MC or TD learning\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-types.png\" />\n",
    "<img width=600 src=\"images/rl-fa-choose.png\" />\n",
    "\n",
    "## 6.1 Incremental Methods\n",
    "\n",
    "### 6.1.1 Gradient Decent \n",
    "<img width=600 src=\"images/rl-fa-gd.png\" />\n",
    "<img width=600 src=\"images/rl-fa-sgd.png\" />\n",
    "\n",
    "### 6.1.2 Linear Function Approximation\n",
    "<img width=600 src=\"images/rl-fa-feature.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-table-lookup-features.png\" />\n",
    "\n",
    "\n",
    "### 6.1.3 Incremental Prediction Algorithms\n",
    "<img width=600 src=\"images/rl-fa-incremental-prediction-algo.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-mc.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td0.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td-lambda2.png\" />\n",
    "\n",
    "\n",
    "### 6.1.4 Incremental Control Algorithms\n",
    "<img width=600 src=\"images/rl-fa-control.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-incremental-control-algo.png\" />\n",
    "\n",
    "\n",
    "### 6.1.5 Convergence\n",
    "<img width=600 src=\"images/rl-fa-convergence-prediction.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-gradient-td.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-control.png\" />\n",
    "\n",
    "## 6.2 Batch Methods\n",
    "### Batch Reinforcement Learning\n",
    "* Gradient descent is simple and appealing\n",
    "* But it is not sample efficient\n",
    "* Batch methods seek to find the best fitting value function \n",
    "* Given the agent’s experience (“training data”)\n",
    "\n",
    "### 6.2.1 Least Squares Prediction\n",
    "<img width=600 src=\"images/rl-fa-lsp.png\" />\n",
    "\n",
    "** Stochastic Gradient Descent with Experience Replay **\n",
    "<img width=600 src=\"images/rl-fa-sgd-with-experience-replay.png\" />\n",
    "<img width=600 src=\"images/rl-fa-dqn.png\" />\n",
    "\n",
    "** Linear Least Squares Prediction **\n",
    "* Experience replay finds least squares solution\n",
    "* But it may take many iterations\n",
    "* Using linear value function approximation $\\hat{v} (s, w) = x(s)^T w$\n",
    "* **We can solve the least squares solution directly**\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo2.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo-convergence.png\" />\n",
    "\n",
    "### 6.2.2 Least Squares Control\n",
    "<img width=600 src=\"images/rl-fa-lsc.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-action-value-fa.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-overview.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lsq.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lspi.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-control-algo-convergence.png\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Policy Gradient Methods\n",
    "** Policy-Based Reinforcement Learning **\n",
    "* In the last lecture we approximated the value or action-value function using parameters θ,\n",
    "$$ V_θ (s) ≈ V^π (s) \\\\\n",
    "Q_θ (s, a) ≈ Q^π (s, a)$$\n",
    "\n",
    "* A policy was generated directly from the value function\n",
    "    * e.g. using $\\epsilon-greedy$\n",
    "  \n",
    "* In this lecture we will directly parametrise the policy\n",
    "$$π_θ (s, a) = \\mathbb{P} [a | s, θ]$$\n",
    "* We will focus again on **model-free** reinforcement learning\n",
    "\n",
    "** Value-Based and Policy-Based RL **\n",
    "<img width=600 src=\"images/rl-pa-types.png\" />\n",
    "* Value Based\n",
    "    * Learnt Value Function\n",
    "    * Implicit policy (e.g. $\\epsilon-greedy$)\n",
    "* Policy Based\n",
    "    * No Value Function\n",
    "    * Learnt Policy\n",
    "* Actor-Critic\n",
    "    * Learnt Value Function\n",
    "    * Learnt Policy\n",
    "\n",
    "** Advantages of Policy-Based RL **\n",
    "* Advantages:\n",
    "    * Better convergence properties\n",
    "    * Effective in high-dimensional or continuous action spaces\n",
    "    * Can learn stochastic policies\n",
    "* Disadvantages:\n",
    "    * Typically converge to a local rather than global optimum\n",
    "    * Evaluating a policy is typically inefficient and high variance\n",
    "    \n",
    "## 7.1 Policy Search\n",
    "** Policy Objective Functions **\n",
    "* Goal: given policy $π_θ (s, a)$ with parameters θ, find best θ\n",
    "* But how do we measure the quality of a policy $π_θ$ ?\n",
    "* In episodic environments we can use the start value\n",
    "$$J_1 (θ) = V^{π_θ} (s_1 ) = \\mathbb{E}_{π_θ} [v_1 ]$$\n",
    "\n",
    "* In continuing environments we can use the average value\n",
    "$$J_{avV} (θ) = \\sum_s d^{π_θ} (s) V^{π_θ} (s)$$\n",
    "\n",
    "* Or the average reward per time-step\n",
    "$$ J_{avR} (θ) = \\sum_s d^{π_θ} (s) \\sum_a \\pi_{\\theta}(s, a) R^a_s $$\n",
    "* where $d^{π_θ} (s)$ is stationary distribution of Markov chain for $π_θ$\n",
    "\n",
    "** Policy Optimisation **\n",
    "* Policy based reinforcement learning is an optimisation problem\n",
    "* Find θ that maximises J(θ)\n",
    "* Some approaches do not use gradient\n",
    "    * Hill climbing\n",
    "    * Simplex / amoeba / Nelder Mead\n",
    "    * Genetic algorithms\n",
    "* Greater efficiency often possible using gradient\n",
    "    * Gradient descent\n",
    "    * Conjugate gradient\n",
    "    * Quasi-newton\n",
    "* We focus on gradient descent, many extensions possible\n",
    "* And on methods that exploit sequential structure\n",
    "\n",
    "## 7.2 Finite Difference Policy Gradient\n",
    "** Policy Gradient **\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient.png\" />\n",
    "\n",
    "** Computing Gradients By Finite Differences **\n",
    "\n",
    "* To evaluate policy gradient of $π_θ (s, a)$\n",
    "* For each dimension k ∈ [1, n]\n",
    "    * Estimate kth partial derivative of objective function w.r.t. θ\n",
    "    * By perturbing θ by small amount in kth dimension\n",
    "$$ \\frac {\\partial J(θ)} {\\partial \\theta_k} \\approx \\frac {J(\\theta + \\epsilon u_k) - J(\\theta)} {\\epsilon} $$\n",
    "    * where u k is unit vector with 1 in kth component, 0 elsewhere\n",
    "    \n",
    "* ses n evaluations to compute policy gradient in n dimensions\n",
    "* **Simple, noisy, inefficient - but sometimes effective**\n",
    "* **Works for arbitrary policies, even if policy is not differentiable**\n",
    "\n",
    "## 7.3 Monte-Carlo Policy Gradient\n",
    "### 7.3.1 Score Function\n",
    "<img width=600 src=\"images/rl-pa-score-function.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-softmax.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-gaussian.png\" />\n",
    "### 7.3.2 Policy Gradient Theorem\n",
    "<img width=600 src=\"images/rl-pa-1step-mdp.png\" />\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient-theorem.png\" />\n",
    "<img width=600 src=\"images/rl-pa-mc-policy-gradient.png\" />\n",
    "\n",
    "\n",
    "## 7.4 Actor-Critic Policy Gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
