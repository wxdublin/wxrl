{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 3\n",
    "\n",
    "# 6 Value Function Approximation\n",
    "\n",
    "## Large-Scale Reinforcement Learning\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "* Backgammon: $10^{20}$ states \n",
    "* Computer Go: $10^{170}$ states \n",
    "* Helicopter: continuous state space\n",
    "How can we **scale up** the model-free methods for prediction and control from the last two lectures?\n",
    "\n",
    "So far we have represented value function by a **lookup table** \n",
    "* Every state s has an entry V(s)\n",
    "* Or every state-action pair s,a has an entry Q(s,a)\n",
    "\n",
    "Problem with large MDPs:\n",
    "* There are too many states and/or actions to store in memory \n",
    "* It is too slow to learn the value of each state individually\n",
    "\n",
    "## Value Function Approximation\n",
    "Solution for large MDPs:\n",
    "* Estimate value function with function approximation\n",
    "$$ \\hat{v} ( s , w ) \\approx v_π ( s ) $$\n",
    "or $$ \\hat{q} ( s , a , w ) \\approx q_π ( s , a )\n",
    "$$\n",
    "* Generalise from seen states to unseen states \n",
    "* Update parameter w using MC or TD learning\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-types.png\" />\n",
    "<img width=600 src=\"images/rl-fa-choose.png\" />\n",
    "\n",
    "## 6.1 Incremental Methods\n",
    "\n",
    "### 6.1.1 Gradient Decent \n",
    "<img width=600 src=\"images/rl-fa-gd.png\" />\n",
    "<img width=600 src=\"images/rl-fa-sgd.png\" />\n",
    "\n",
    "### 6.1.2 Linear Function Approximation\n",
    "<img width=600 src=\"images/rl-fa-feature.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-table-lookup-features.png\" />\n",
    "\n",
    "\n",
    "### 6.1.3 Incremental Prediction Algorithms\n",
    "<img width=600 src=\"images/rl-fa-incremental-prediction-algo.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-mc.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td0.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td-lambda2.png\" />\n",
    "\n",
    "\n",
    "### 6.1.4 Incremental Control Algorithms\n",
    "<img width=600 src=\"images/rl-fa-control.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-incremental-control-algo.png\" />\n",
    "\n",
    "\n",
    "### 6.1.5 Convergence\n",
    "<img width=600 src=\"images/rl-fa-convergence-prediction.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-gradient-td.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-control.png\" />\n",
    "\n",
    "## 6.2 Batch Methods\n",
    "### Batch Reinforcement Learning\n",
    "* Gradient descent is simple and appealing\n",
    "* But it is not sample efficient\n",
    "* Batch methods seek to find the best fitting value function \n",
    "* Given the agent’s experience (“training data”)\n",
    "\n",
    "### 6.2.1 Least Squares Prediction\n",
    "<img width=600 src=\"images/rl-fa-lsp.png\" />\n",
    "\n",
    "** Stochastic Gradient Descent with Experience Replay **\n",
    "<img width=600 src=\"images/rl-fa-sgd-with-experience-replay.png\" />\n",
    "<img width=600 src=\"images/rl-fa-dqn.png\" />\n",
    "\n",
    "** Linear Least Squares Prediction **\n",
    "* Experience replay finds least squares solution\n",
    "* But it may take many iterations\n",
    "* Using linear value function approximation $\\hat{v} (s, w) = x(s)^T w$\n",
    "* **We can solve the least squares solution directly**\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo2.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo-convergence.png\" />\n",
    "\n",
    "### 6.2.2 Least Squares Control\n",
    "<img width=600 src=\"images/rl-fa-lsc.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-action-value-fa.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-overview.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lsq.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lspi.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-control-algo-convergence.png\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Policy Gradient Methods\n",
    "** Policy-Based Reinforcement Learning **\n",
    "* In the last lecture we approximated the value or action-value function using parameters θ,\n",
    "$$ V_θ (s) ≈ V^π (s) \\\\\n",
    "Q_θ (s, a) ≈ Q^π (s, a)$$\n",
    "\n",
    "* A policy was generated directly from the value function\n",
    "    * e.g. using $\\epsilon-greedy$\n",
    "  \n",
    "* In this lecture we will directly parametrise the policy\n",
    "$$π_θ (s, a) = \\mathbb{P} [a | s, θ]$$\n",
    "* We will focus again on **model-free** reinforcement learning\n",
    "\n",
    "** Value-Based and Policy-Based RL **\n",
    "<img width=600 src=\"images/rl-pa-types.png\" />\n",
    "* Value Based\n",
    "    * Learnt Value Function\n",
    "    * Implicit policy (e.g. $\\epsilon-greedy$)\n",
    "* Policy Based\n",
    "    * No Value Function\n",
    "    * Learnt Policy\n",
    "* Actor-Critic\n",
    "    * Learnt Value Function\n",
    "    * Learnt Policy\n",
    "\n",
    "** Advantages of Policy-Based RL **\n",
    "* Advantages:\n",
    "    * Better convergence properties\n",
    "    * Effective in high-dimensional or continuous action spaces\n",
    "    * Can learn stochastic policies\n",
    "* Disadvantages:\n",
    "    * Typically converge to a local rather than global optimum\n",
    "    * Evaluating a policy is typically inefficient and high variance\n",
    "    \n",
    "## 7.1 Policy Search\n",
    "** Policy Objective Functions **\n",
    "* Goal: given policy $π_θ (s, a)$ with parameters θ, find best θ\n",
    "* But how do we measure the quality of a policy $π_θ$ ?\n",
    "* In episodic environments we can use the start value\n",
    "$$J_1 (θ) = V^{π_θ} (s_1 ) = \\mathbb{E}_{π_θ} [v_1 ]$$\n",
    "\n",
    "* In continuing environments we can use the average value\n",
    "$$J_{avV} (θ) = \\sum_s d^{π_θ} (s) V^{π_θ} (s)$$\n",
    "\n",
    "* Or the average reward per time-step\n",
    "$$ J_{avR} (θ) = \\sum_s d^{π_θ} (s) \\sum_a \\pi_{\\theta}(s, a) R^a_s $$\n",
    "* where $d^{π_θ} (s)$ is stationary distribution of Markov chain for $π_θ$\n",
    "\n",
    "** Policy Optimisation **\n",
    "* Policy based reinforcement learning is an optimisation problem\n",
    "* Find θ that maximises J(θ)\n",
    "* Some approaches do not use gradient\n",
    "    * Hill climbing\n",
    "    * Simplex / amoeba / Nelder Mead\n",
    "    * Genetic algorithms\n",
    "* Greater efficiency often possible using gradient\n",
    "    * Gradient descent\n",
    "    * Conjugate gradient\n",
    "    * Quasi-newton\n",
    "* We focus on gradient descent, many extensions possible\n",
    "* And on methods that exploit sequential structure\n",
    "\n",
    "## 7.2 Finite Difference Policy Gradient\n",
    "** Policy Gradient **\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient.png\" />\n",
    "\n",
    "** Computing Gradients By Finite Differences **\n",
    "\n",
    "* To evaluate policy gradient of $π_θ (s, a)$\n",
    "* For each dimension k ∈ [1, n]\n",
    "    * Estimate kth partial derivative of objective function w.r.t. θ\n",
    "    * By perturbing θ by small amount in kth dimension\n",
    "$$ \\frac {\\partial J(θ)} {\\partial \\theta_k} \\approx \\frac {J(\\theta + \\epsilon u_k) - J(\\theta)} {\\epsilon} $$\n",
    "    * where u k is unit vector with 1 in kth component, 0 elsewhere\n",
    "    \n",
    "* ses n evaluations to compute policy gradient in n dimensions\n",
    "* **Simple, noisy, inefficient - but sometimes effective**\n",
    "* **Works for arbitrary policies, even if policy is not differentiable**\n",
    "\n",
    "## 7.3 Monte-Carlo Policy Gradient (REINFORCE)\n",
    "### 7.3.1 Score Function\n",
    "<img width=600 src=\"images/rl-pa-score-function.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-softmax.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-gaussian.png\" />\n",
    "### 7.3.2 Policy Gradient Theorem\n",
    "<img width=600 src=\"images/rl-pa-1step-mdp.png\" />\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient-theorem.png\" />\n",
    "### Monte-Carlo Policy Gradient (REINFORCE)\n",
    "<img width=600 src=\"images/rl-pa-mc-policy-gradient.png\" />\n",
    "\n",
    "\n",
    "## 7.4 Actor-Critic Policy Gradient\n",
    "\n",
    "### 7.4.1 Q Actor-Critic\n",
    "\n",
    "** Reducing Variance Using a Critic **\n",
    "<img width=600 src=\"images/rl-pa-actor-critic.png\" />\n",
    "\n",
    "** Estimating the Action-Value Function **\n",
    "* The critic is solving a familiar problem: policy evaluation\n",
    "* How good is policy $π_θ$ for current parameters θ?\n",
    "* This problem was explored in previous two lectures, e.g.\n",
    "    * Monte-Carlo policy evaluation\n",
    "    * Temporal-Difference learning\n",
    "    * TD(λ)\n",
    "* Could also use e.g. least-squares policy evaluation\n",
    "\n",
    "** Q Actor-Critic **\n",
    "<img width=600 src=\"images/rl-pa-qac.png\" />\n",
    "\n",
    "### 7.4.2 Advantage Actor-Critic\n",
    "\n",
    "** Bias in Actor-Critic Algorithms **\n",
    "\n",
    "* Approximating the policy gradient introduces bias\n",
    "* A biased policy gradient may not find the right solution\n",
    "    * e.g. if Q w (s, a) uses aliased features, can we solve gridworld example?\n",
    "* Luckily, if we choose value function approximation carefully\n",
    "* Then we can avoid introducing any bias\n",
    "* i.e. We can still follow the exact policy gradient\n",
    "\n",
    "** Compatible Function Approximation Theorem **\n",
    "\n",
    "If the following two conditions are satisfied:\n",
    "1. Value function approximator is **compatible** to the policy\n",
    "$$ \\triangledown_w Q_w (s, a) = \\triangledown_\\theta log_{π_{\\theta}} (s, a)$$\n",
    "2. Value function parameters w minimise the mean-squared error\n",
    "$$ ε = \\mathbb{E}_{π_θ} (Q_{π_θ} (s, a) − Q_w (s, a))^2 $$\n",
    "Then the policy gradient is exact,\n",
    "$$∇_θ J(θ) = E_{π_θ} [∇_θ log_{π_θ} (s, a) Q_w (s, a)] $$\n",
    "\n",
    "<img width=600 src=\"images/rl-pa-aac-reduce-variance.png\" />\n",
    "<img width=600 src=\"images/rl-pa-aac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-aac2.png\" />\n",
    "\n",
    "### 7.4.3 TD Actor-Critic\n",
    "<img width=600 src=\"images/rl-pa-td-ac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-td-ac2.png\" />\n",
    "<img width=600 src=\"images/rl-pa-td-ac3.png\" />\n",
    "\n",
    "### 7.4.4 Natural Actor-Critic\n",
    "<img width=600 src=\"images/rl-pa-alternative-direction.png\" />\n",
    "<img width=600 src=\"images/rl-pa-natural-ac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-natural-ac2.png\" />\n",
    "\n",
    "** Summary of Policy Gradient Algorithms **\n",
    "<img width=600 src=\"images/rl-pa-algorithms.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Integrating Learning and Planning\n",
    "\n",
    "## 8.1 Introduction\n",
    "** Model-Based Reinforcement Learning **\n",
    "* Last lecture: learn policy directly from experience\n",
    "* Previous lectures: learn value function directly from experience\n",
    "* This lecture: learn model directly from experience\n",
    "* and use planning to construct a value function or policy\n",
    "* Integrate learning and planning into a single architecture\n",
    "\n",
    "* Model-Free RL\n",
    "    * No model\n",
    "    * Learn value function (and/or policy) from experience\n",
    "* Model-Based RL\n",
    "    * Learn a model from experience\n",
    "    * Plan value function (and/or policy) from model\n",
    "\n",
    "* Advantages:\n",
    "    * Can efficiently learn model by supervised learning methods\n",
    "    * Can reason about model uncertainty\n",
    "* Disadvantages:\n",
    "    * First learn a model, then construct a value function\n",
    "    * two sources of approximation error\n",
    "\n",
    "<img width=600 src=\"images/rl-model-based.png\" />\n",
    "\n",
    "## 8.2 Model-Based Reinforcement Learning\n",
    "\n",
    "** What is a Model? **\n",
    "* A model $\\mathcal{M}$ is a representation of an MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}>$ , parametrized by η\n",
    "* We will assume state space $\\mathcal{S}$ and action space $\\mathcal{A}$ are known\n",
    "* So a model $M = <\\mathcal{P}_η , \\mathcal{R}_η>$ represents state transitions $\\mathcal{P}_η ≈ \\mathcal{P}$ and rewards $\\mathcal{R}_η ≈ \\mathcal{R}$\n",
    "$$ S_{t+1} ∼ P_η (S_{t+1} | S_t , A_t ) \\\\\n",
    "R_{t+1} = R_η (R_{t+1} | S_t , A_t ) $$\n",
    "* Typically assume conditional independence between state transitions and rewards\n",
    "$$ P [S_{t+1} , R_{t+1} | S_t , A_t ] = P [S_{t+1} | S_t , A_t ] P [R_{t+1} | S_t , A_t ]$$\n",
    "\n",
    "** Model Learning **\n",
    "* Goal: estimate model $M_η$ from experience {S_1 , A_1 , R_2 , ..., S_T }\n",
    "* This is a supervised learning problem\n",
    "$$\n",
    "S_1 , A_1 → R_2 , S_2\n",
    "S_2 , A_2 → R_3 , S_3\n",
    ".\n",
    ".\n",
    "S_{T −1} , A_{T −1} → R_T , S_T\n",
    "$$\n",
    "* Learning s, a → r is a **regression problem**\n",
    "* Learning s, a → s is a **density estimation problem**\n",
    "* Pick loss function, e.g. **mean-squared error, KL divergence**, ...\n",
    "* Find parameters η that minimise **empirical loss**\n",
    "\n",
    "** Examples of Models **\n",
    "* Table Lookup Model\n",
    "* Linear Expectation Model\n",
    "* Linear Gaussian Model\n",
    "* Gaussian Process Model\n",
    "* Deep Belief Network Model\n",
    "* ...\n",
    "\n",
    "** Table Lookup Model **\n",
    "* Model is an explicit MDP, $\\hat{P},\\hat{R}$\n",
    "* Count visits N(s, a) to each state action pair\n",
    "$$ \\hat{P}^a_{s,s'} = \\frac{1}{N(s,a)}\\sum_{t=1}^{T}\\mathbf{1}(S_t, A_t, S_{t+1} = s,a,s') \\\\\n",
    "\\hat{R}^a_s = \\frac{1}{N(s,a)}\\sum_{t=1}^{T}\\mathbf{1}(S_t, A_t = s, a)R_t \\\\\n",
    "$$\n",
    "* Alternatively\n",
    "    * At each time-step t, record experience tuple $<S_t , A_t , R_{t+1} , S_{t+1}>$\n",
    "    * To sample model, randomly pick tuple matching $<s, a, ·, ·>$\n",
    "\n",
    "### Planning with a Model \n",
    "* Given a model $M_η = <P_η , R_η>$\n",
    "* Solve the MDP $<S, A, P_η , R_η>$\n",
    "* Using favorite planning algorithm\n",
    "    * Value iteration\n",
    "    * Policy iteration\n",
    "    * Tree search\n",
    "    * ...\n",
    "    \n",
    "** Sample-Based Planning **\n",
    "* A simple but powerful approach to planning\n",
    "* Use the model only to generate samples\n",
    "* Sample experience from model\n",
    "$$ S_{t+1} ∼ P_η (S_{t+1} | S_t , A_t ) \\\\\n",
    "R_{t+1} = R_η (R_{t+1} | S_t , A_t ) $$\n",
    "* Apply model-free RL to samples, e.g.:\n",
    "    * Monte-Carlo control\n",
    "    * Sarsa\n",
    "    * Q-learning\n",
    "* Sample-based planning methods are often more efficient\n",
    "\n",
    "** Planning with an Inaccurate Model **\n",
    "* Given an imperfect model $<P_η , R_η = P, R>$\n",
    "* Performance of model-based RL is limited to optimal policy for approximate MDP $<S, A, P_η , R_η>$\n",
    "* i.e. Model-based RL is only as good as the estimated model\n",
    "* When the model is inaccurate, planning process will compute a suboptimal policy\n",
    "* Solution 1: when model is wrong, use model-free RL\n",
    "* Solution 2: reason explicitly about model uncertainty\n",
    "\n",
    "## 8.3 Integrated Architectures\n",
    "\n",
    "### 8.3.1 Dyna\n",
    "** Real and Simulated Experience **\n",
    "* We consider two sources of experience\n",
    "* **Real experience** Sampled from environment (true MDP)\n",
    "$$ S ∼ P^a_{s,s'} \\\\\n",
    "R = R^a_s $$\n",
    "* **Simulated experience** Sampled from model (approximate MDP)\n",
    "$$ S' ∼ P_η (S' | S, A) \\\\\n",
    "R = R_η (R | S, A) $$\n",
    "\n",
    "** Integrating Learning and Planning **\n",
    "* Model-Free RL\n",
    "    * No model\n",
    "    * Learn value function (and/or policy) from real experience\n",
    "    \n",
    "* Model-Based RL (using Sample-Based Planning)\n",
    "    * Learn a model from **real experience**\n",
    "    * Plan value function (and/or policy) from **simulated experience**\n",
    "* Dyna\n",
    "    * Learn a model from real experience\n",
    "    * **Learn and plan value function (and/or policy) from real and simulated experience**\n",
    "\n",
    "<img width=600 src=\"images/rl-model-dyna.png\" />\n",
    "<img width=600 src=\"images/rl-model-dyna-algo.png\" />\n",
    "\n",
    "## 8.4 Simulation-Based Search\n",
    "### 8.4.1 Simulation-Based Search\n",
    "<img width=600 src=\"images/rl-model-forward-search.png\" />\n",
    "<img width=600 src=\"images/rl-model-simulation-based-search.png\" />\n",
    "\n",
    "* Simulate episodes of experience from now with the model\n",
    "$$ \\left\\{ s_t^k , A_k^t , R_{t+1}^k, ..., S_T^k \\right\\}_{k=1}^K \\thicksim M_ν $$\n",
    "* Apply model-free RL to simulated episodes\n",
    "    * Monte-Carlo control → Monte-Carlo search\n",
    "    * Sarsa → TD search\n",
    "    \n",
    "### 8.4.2 Monte-Carlo Tree Search\n",
    "** Simple Monte-Carlo Search **\n",
    "* Given a model $M_ν$ and a **simulation policy** π\n",
    "* For each action a ∈ A\n",
    "    * Simulate K episodes from current (real) state $s_t$\n",
    "$$\\left\\{s_t , a, R_{t+1}^k, S_{t+1}^k, A^k_{t+1} , ..., S_T^k \\right\\}^K_{k=1} \\sim M_ν , π $$\n",
    "    * Evaluate actions by mean return (Monte-Carlo evaluation)\n",
    "$$ Q(s_t , a) = \\frac{1}{K} \\sum^K_{k=1}G_t \\overset{P}{\\to} q_π (s_t , a) $$\n",
    "* Select current (real) action with maximum value\n",
    "$$a_t = \\underset{a \\in A}{argmax} Q(s_t , a)$$\n",
    "\n",
    "** Monte-Carlo Tree Search (Evaluation) **\n",
    "* Given a model $M_ν$\n",
    "* Simulate K episodes from current state $s_t$ using current simulation policy π\n",
    "$$\\left\\{s_t , A^k_t , R_{t+1}^k, S_{t+1}^k, ..., S _T^k \\right\\}^K_{k=1} \\sim M_ν , π$$\n",
    "* Build a search tree containing visited states and actions\n",
    "* Evaluate states Q(s, a) by mean return of episodes from s, a\n",
    "$$ Q(s, a) =\\frac{1}{N(s, a)}\\sum^K_{k=1}\\sum^T_{u=t}\\mathbf{1}(S_u, A_u = s, a)G_u \\overset{P}{\\to} q_{\\pi}(s,a)$$\n",
    "* After search is finished, select current (real) action with maximum value in search tree\n",
    "$$a_t = \\underset{a \\in A}{argmax} Q(s_t , a)$$\n",
    "\n",
    "** Monte-Carlo Tree Search (Simulation) **\n",
    "* In MCTS, the simulation policy π improves\n",
    "* Each simulation consists of two phases (in-tree, out-of-tree)\n",
    "    * Tree policy (improves): pick actions to maximise Q(S, A)\n",
    "    * Default policy (fixed): pick actions randomly\n",
    "* Repeat (each simulation)\n",
    "    * Evaluate states Q(S, A) by Monte-Carlo evaluation\n",
    "    * Improve tree policy, e.g. by $\\epsilon − greedy(Q)$\n",
    "* Monte-Carlo control applied to simulated experience\n",
    "* Converges on the optimal search tree, $Q(S, A) → q_∗ (S, A)$\n",
    "\n",
    "** Advantages of MC Tree Search **\n",
    "* Highly selective best-first search\n",
    "* Evaluates states dynamically (unlike e.g. DP)\n",
    "* Uses sampling to break curse of dimensionality\n",
    "* Works for “black-box” models (only requires samples)\n",
    "* Computationally efficient, anytime, parallelisable\n",
    "\n",
    "### 8.4.3 MCTS in Go\n",
    "### 8.4.4 Temporal-Difference Search\n",
    "** Temporal-Difference Search **\n",
    "* Simulation-based search\n",
    "* Using TD instead of MC (bootstrapping)\n",
    "* MC tree search applies MC control to sub-MDP from now\n",
    "* TD search applies Sarsa to sub-MDP from now\n",
    "\n",
    "** MC vs. TD search **\n",
    "* For model-free reinforcement learning, bootstrapping is helpful\n",
    "    * TD learning reduces variance but increases bias\n",
    "    * TD learning is usually more efficient than MC\n",
    "    * TD(λ) can be much more efficient than MC\n",
    "\n",
    "* For simulation-based search, bootstrapping is also helpful\n",
    "    * TD search reduces variance but increases bias\n",
    "    * TD search is usually more efficient than MC search\n",
    "    * TD(λ) search can be much more efficient than MC search\n",
    "\n",
    "** TD Search **\n",
    "* Simulate episodes from the current (real) state $s_t$\n",
    "* Estimate action-value function Q(s, a)\n",
    "* For each step of simulation, update action-values by Sarsa\n",
    "$$ ∆Q(S, A) = α(R + γQ(S' , A' ) − Q(S, A))$$\n",
    "* Select actions based on action-values Q(s, a) e.g. $\\epsilon-greedy$\n",
    "* May also use function approximation for Q\n",
    "\n",
    "** Dyna-2 **\n",
    "* In Dyna-2, the agent stores two sets of feature weights\n",
    "    * Long-term memory\n",
    "    * Short-term (working) memory\n",
    "* Long-term memory is updated from real experience using TD learning\n",
    "    * General **domain knowledge** that applies to any episode\n",
    "* Short-term memory is updated from simulated experience using TD search\n",
    "    * Specific **local knowledge** about the current situation\n",
    "* Over value function is sum of long and short-term memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
