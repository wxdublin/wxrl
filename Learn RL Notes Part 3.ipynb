{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Value Function Approximation\n",
    "\n",
    "## Large-Scale Reinforcement Learning\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "* Backgammon: $10^{20}$ states \n",
    "* Computer Go: $10^{170}$ states \n",
    "* Helicopter: continuous state space\n",
    "How can we **scale up** the model-free methods for prediction and control from the last two lectures?\n",
    "\n",
    "So far we have represented value function by a **lookup table** \n",
    "* Every state s has an entry V(s)\n",
    "* Or every state-action pair s,a has an entry Q(s,a)\n",
    "\n",
    "Problem with large MDPs:\n",
    "* There are too many states and/or actions to store in memory \n",
    "* It is too slow to learn the value of each state individually\n",
    "\n",
    "## Value Function Approximation\n",
    "Solution for large MDPs:\n",
    "* Estimate value function with function approximation\n",
    "$$ \\hat{v} ( s , w ) \\approx v_π ( s ) $$\n",
    "or $$ \\hat{q} ( s , a , w ) \\approx q_π ( s , a )\n",
    "$$\n",
    "* Generalise from seen states to unseen states \n",
    "* Update parameter w using MC or TD learning\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-types.png\" />\n",
    "<img width=600 src=\"images/rl-fa-choose.png\" />\n",
    "\n",
    "## 6.1 Incremental Methods\n",
    "\n",
    "### 6.1.1 Gradient Decent \n",
    "<img width=600 src=\"images/rl-fa-gd.png\" />\n",
    "<img width=600 src=\"images/rl-fa-sgd.png\" />\n",
    "\n",
    "### 6.1.2 Linear Function Approximation\n",
    "<img width=600 src=\"images/rl-fa-feature.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-table-lookup-features.png\" />\n",
    "\n",
    "\n",
    "### 6.1.3 Incremental Prediction Algorithms\n",
    "<img width=600 src=\"images/rl-fa-incremental-prediction-algo.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-mc.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td0.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td-lambda2.png\" />\n",
    "\n",
    "\n",
    "### 6.1.4 Incremental Control Algorithms\n",
    "<img width=600 src=\"images/rl-fa-control.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-incremental-control-algo.png\" />\n",
    "\n",
    "\n",
    "### 6.1.5 Convergence\n",
    "<img width=600 src=\"images/rl-fa-convergence-prediction.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-gradient-td.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-control.png\" />\n",
    "\n",
    "## 6.2 Batch Methods\n",
    "### Batch Reinforcement Learning\n",
    "* Gradient descent is simple and appealing\n",
    "* But it is not sample efficient\n",
    "* Batch methods seek to find the best fitting value function \n",
    "* Given the agent’s experience (“training data”)\n",
    "\n",
    "### 6.2.1 Least Squares Prediction\n",
    "\n",
    "### 6.2.2 Least Squares Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Policy Gradient Methods\n",
    "# 8 Integrating Learning and Planning\n",
    "# 9 Exploration and Exploitation\n",
    "# 10 Case study - RL in games\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
