{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RL Notes Part 3\n",
    "\n",
    "# 6 Value Function Approximation\n",
    "\n",
    "## Large-Scale Reinforcement Learning\n",
    "Reinforcement learning can be used to solve large problems, e.g.\n",
    "* Backgammon: $10^{20}$ states \n",
    "* Computer Go: $10^{170}$ states \n",
    "* Helicopter: continuous state space\n",
    "How can we **scale up** the model-free methods for prediction and control from the last two lectures?\n",
    "\n",
    "So far we have represented value function by a **lookup table** \n",
    "* Every state s has an entry V(s)\n",
    "* Or every state-action pair s,a has an entry Q(s,a)\n",
    "\n",
    "Problem with large MDPs:\n",
    "* There are too many states and/or actions to store in memory \n",
    "* It is too slow to learn the value of each state individually\n",
    "\n",
    "## Value Function Approximation\n",
    "Solution for large MDPs:\n",
    "* Estimate value function with function approximation\n",
    "$$ \\hat{v} ( s , w ) \\approx v_π ( s ) $$\n",
    "or $$ \\hat{q} ( s , a , w ) \\approx q_π ( s , a )\n",
    "$$\n",
    "* Generalise from seen states to unseen states \n",
    "* Update parameter w using MC or TD learning\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-types.png\" />\n",
    "<img width=600 src=\"images/rl-fa-choose.png\" />\n",
    "\n",
    "## 6.1 Incremental Methods\n",
    "\n",
    "### 6.1.1 Gradient Decent \n",
    "<img width=600 src=\"images/rl-fa-gd.png\" />\n",
    "<img width=600 src=\"images/rl-fa-sgd.png\" />\n",
    "\n",
    "### 6.1.2 Linear Function Approximation\n",
    "<img width=600 src=\"images/rl-fa-feature.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-table-lookup-features.png\" />\n",
    "\n",
    "\n",
    "### 6.1.3 Incremental Prediction Algorithms\n",
    "<img width=600 src=\"images/rl-fa-incremental-prediction-algo.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-mc.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td0.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-td-lambda2.png\" />\n",
    "\n",
    "\n",
    "### 6.1.4 Incremental Control Algorithms\n",
    "<img width=600 src=\"images/rl-fa-control.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-linear-action-value-fa.png\" />\n",
    "\n",
    "<img width=600 src=\"images/rl-fa-incremental-control-algo.png\" />\n",
    "\n",
    "\n",
    "### 6.1.5 Convergence\n",
    "<img width=600 src=\"images/rl-fa-convergence-prediction.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-gradient-td.png\" />\n",
    "<img width=600 src=\"images/rl-fa-convergence-control.png\" />\n",
    "\n",
    "## 6.2 Batch Methods\n",
    "### Batch Reinforcement Learning\n",
    "* Gradient descent is simple and appealing\n",
    "* But it is not sample efficient\n",
    "* Batch methods seek to find the best fitting value function \n",
    "* Given the agent’s experience (“training data”)\n",
    "\n",
    "### 6.2.1 Least Squares Prediction\n",
    "<img width=600 src=\"images/rl-fa-lsp.png\" />\n",
    "\n",
    "** Stochastic Gradient Descent with Experience Replay **\n",
    "<img width=600 src=\"images/rl-fa-sgd-with-experience-replay.png\" />\n",
    "<img width=600 src=\"images/rl-fa-dqn.png\" />\n",
    "\n",
    "** Linear Least Squares Prediction **\n",
    "* Experience replay finds least squares solution\n",
    "* But it may take many iterations\n",
    "* Using linear value function approximation $\\hat{v} (s, w) = x(s)^T w$\n",
    "* **We can solve the least squares solution directly**\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo2.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsp-linear-algo-convergence.png\" />\n",
    "\n",
    "### 6.2.2 Least Squares Control\n",
    "<img width=600 src=\"images/rl-fa-lsc.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-action-value-fa.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-overview.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lsq.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-lspi.png\" />\n",
    "<img width=600 src=\"images/rl-fa-lsc-control-algo-convergence.png\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Policy Gradient Methods\n",
    "** Policy-Based Reinforcement Learning **\n",
    "* In the last lecture we approximated the value or action-value function using parameters θ,\n",
    "$$ V_θ (s) ≈ V^π (s) \\\\\n",
    "Q_θ (s, a) ≈ Q^π (s, a)$$\n",
    "\n",
    "* A policy was generated directly from the value function\n",
    "    * e.g. using $\\epsilon-greedy$\n",
    "  \n",
    "* In this lecture we will directly parametrise the policy\n",
    "$$π_θ (s, a) = \\mathbb{P} [a | s, θ]$$\n",
    "* We will focus again on **model-free** reinforcement learning\n",
    "\n",
    "** Value-Based and Policy-Based RL **\n",
    "<img width=600 src=\"images/rl-pa-types.png\" />\n",
    "* Value Based\n",
    "    * Learnt Value Function\n",
    "    * Implicit policy (e.g. $\\epsilon-greedy$)\n",
    "* Policy Based\n",
    "    * No Value Function\n",
    "    * Learnt Policy\n",
    "* Actor-Critic\n",
    "    * Learnt Value Function\n",
    "    * Learnt Policy\n",
    "\n",
    "** Advantages of Policy-Based RL **\n",
    "* Advantages:\n",
    "    * Better convergence properties\n",
    "    * Effective in high-dimensional or continuous action spaces\n",
    "    * Can learn stochastic policies\n",
    "* Disadvantages:\n",
    "    * Typically converge to a local rather than global optimum\n",
    "    * Evaluating a policy is typically inefficient and high variance\n",
    "    \n",
    "## 7.1 Policy Search\n",
    "** Policy Objective Functions **\n",
    "* Goal: given policy $π_θ (s, a)$ with parameters θ, find best θ\n",
    "* But how do we measure the quality of a policy $π_θ$ ?\n",
    "* In episodic environments we can use the start value\n",
    "$$J_1 (θ) = V^{π_θ} (s_1 ) = \\mathbb{E}_{π_θ} [v_1 ]$$\n",
    "\n",
    "* In continuing environments we can use the average value\n",
    "$$J_{avV} (θ) = \\sum_s d^{π_θ} (s) V^{π_θ} (s)$$\n",
    "\n",
    "* Or the average reward per time-step\n",
    "$$ J_{avR} (θ) = \\sum_s d^{π_θ} (s) \\sum_a \\pi_{\\theta}(s, a) R^a_s $$\n",
    "* where $d^{π_θ} (s)$ is stationary distribution of Markov chain for $π_θ$\n",
    "\n",
    "** Policy Optimisation **\n",
    "* Policy based reinforcement learning is an optimisation problem\n",
    "* Find θ that maximises J(θ)\n",
    "* Some approaches do not use gradient\n",
    "    * Hill climbing\n",
    "    * Simplex / amoeba / Nelder Mead\n",
    "    * Genetic algorithms\n",
    "* Greater efficiency often possible using gradient\n",
    "    * Gradient descent\n",
    "    * Conjugate gradient\n",
    "    * Quasi-newton\n",
    "* We focus on gradient descent, many extensions possible\n",
    "* And on methods that exploit sequential structure\n",
    "\n",
    "## 7.2 Finite Difference Policy Gradient\n",
    "** Policy Gradient **\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient.png\" />\n",
    "\n",
    "** Computing Gradients By Finite Differences **\n",
    "\n",
    "* To evaluate policy gradient of $π_θ (s, a)$\n",
    "* For each dimension k ∈ [1, n]\n",
    "    * Estimate kth partial derivative of objective function w.r.t. θ\n",
    "    * By perturbing θ by small amount in kth dimension\n",
    "$$ \\frac {\\partial J(θ)} {\\partial \\theta_k} \\approx \\frac {J(\\theta + \\epsilon u_k) - J(\\theta)} {\\epsilon} $$\n",
    "    * where u k is unit vector with 1 in kth component, 0 elsewhere\n",
    "    \n",
    "* ses n evaluations to compute policy gradient in n dimensions\n",
    "* **Simple, noisy, inefficient - but sometimes effective**\n",
    "* **Works for arbitrary policies, even if policy is not differentiable**\n",
    "\n",
    "## 7.3 Monte-Carlo Policy Gradient (REINFORCE)\n",
    "### 7.3.1 Score Function\n",
    "<img width=600 src=\"images/rl-pa-score-function.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-softmax.png\" />\n",
    "<img width=600 src=\"images/rl-pa-score-function-gaussian.png\" />\n",
    "### 7.3.2 Policy Gradient Theorem\n",
    "<img width=600 src=\"images/rl-pa-1step-mdp.png\" />\n",
    "<img width=600 src=\"images/rl-pa-policy-gradient-theorem.png\" />\n",
    "### Monte-Carlo Policy Gradient (REINFORCE)\n",
    "<img width=600 src=\"images/rl-pa-mc-policy-gradient.png\" />\n",
    "\n",
    "\n",
    "## 7.4 Actor-Critic Policy Gradient\n",
    "\n",
    "### 7.4.1 Q Actor-Critic\n",
    "\n",
    "** Reducing Variance Using a Critic **\n",
    "<img width=600 src=\"images/rl-pa-actor-critic.png\" />\n",
    "\n",
    "** Estimating the Action-Value Function **\n",
    "* The critic is solving a familiar problem: policy evaluation\n",
    "* How good is policy $π_θ$ for current parameters θ?\n",
    "* This problem was explored in previous two lectures, e.g.\n",
    "    * Monte-Carlo policy evaluation\n",
    "    * Temporal-Difference learning\n",
    "    * TD(λ)\n",
    "* Could also use e.g. least-squares policy evaluation\n",
    "\n",
    "** Q Actor-Critic **\n",
    "<img width=600 src=\"images/rl-pa-qac.png\" />\n",
    "\n",
    "### 7.4.2 Advantage Actor-Critic\n",
    "\n",
    "** Bias in Actor-Critic Algorithms **\n",
    "\n",
    "* Approximating the policy gradient introduces bias\n",
    "* A biased policy gradient may not find the right solution\n",
    "    * e.g. if Q w (s, a) uses aliased features, can we solve gridworld example?\n",
    "* Luckily, if we choose value function approximation carefully\n",
    "* Then we can avoid introducing any bias\n",
    "* i.e. We can still follow the exact policy gradient\n",
    "\n",
    "** Compatible Function Approximation Theorem **\n",
    "\n",
    "If the following two conditions are satisfied:\n",
    "1. Value function approximator is **compatible** to the policy\n",
    "$$ \\triangledown_w Q_w (s, a) = \\triangledown_\\theta log_{π_{\\theta}} (s, a)$$\n",
    "2. Value function parameters w minimise the mean-squared error\n",
    "$$ ε = \\mathbb{E}_{π_θ} (Q_{π_θ} (s, a) − Q_w (s, a))^2 $$\n",
    "Then the policy gradient is exact,\n",
    "$$∇_θ J(θ) = E_{π_θ} [∇_θ log_{π_θ} (s, a) Q_w (s, a)] $$\n",
    "\n",
    "<img width=600 src=\"images/rl-pa-aac-reduce-variance.png\" />\n",
    "<img width=600 src=\"images/rl-pa-aac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-aac2.png\" />\n",
    "\n",
    "### 7.4.3 TD Actor-Critic\n",
    "<img width=600 src=\"images/rl-pa-td-ac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-td-ac2.png\" />\n",
    "<img width=600 src=\"images/rl-pa-td-ac3.png\" />\n",
    "\n",
    "### 7.4.4 Natural Actor-Critic\n",
    "<img width=600 src=\"images/rl-pa-alternative-direction.png\" />\n",
    "<img width=600 src=\"images/rl-pa-natural-ac.png\" />\n",
    "<img width=600 src=\"images/rl-pa-natural-ac2.png\" />\n",
    "\n",
    "** Summary of Policy Gradient Algorithms **\n",
    "<img width=600 src=\"images/rl-pa-algorithms.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Integrating Learning and Planning\n",
    "\n",
    "## 8.1 Introduction\n",
    "** Model-Based Reinforcement Learning **\n",
    "* Last lecture: learn policy directly from experience\n",
    "* Previous lectures: learn value function directly from experience\n",
    "* This lecture: learn model directly from experience\n",
    "* and use planning to construct a value function or policy\n",
    "* Integrate learning and planning into a single architecture\n",
    "\n",
    "* Model-Free RL\n",
    "    * No model\n",
    "    * Learn value function (and/or policy) from experience\n",
    "* Model-Based RL\n",
    "    * Learn a model from experience\n",
    "    * Plan value function (and/or policy) from model\n",
    "\n",
    "* Advantages:\n",
    "    * Can efficiently learn model by supervised learning methods\n",
    "    * Can reason about model uncertainty\n",
    "* Disadvantages:\n",
    "    * First learn a model, then construct a value function\n",
    "    * two sources of approximation error\n",
    "\n",
    "<img width=600 src=\"images/rl-model-based.png\" />\n",
    "\n",
    "## 8.2 Model-Based Reinforcement Learning\n",
    "## 8.3 Integrated Architectures\n",
    "## 8.4 Simulation-Based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Exploration and Exploitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Case study - RL in games\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
